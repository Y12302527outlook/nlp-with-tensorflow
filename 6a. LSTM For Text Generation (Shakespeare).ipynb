{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Introduction\n",
    "\n",
    "Implementation of RNNs for sentence generation using Keras. The reference taken is [this](https://chunml.github.io/ChunML.github.io/project/Creating-Text-Generator-Using-Recurrent-Neural-Network/) website. The dataset use is Andrej Karpathy's [page](https://cs.stanford.edu/people/karpathy/char-rnn/)\n",
    "\n",
    "First we will download the text if required and load the data as characters in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Shakespeare/shakespeare_input.txt exists, not downloading\n",
      "Number of unique characters are 67\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "def maybe_download(target_url, local_dir, local_file):\n",
    "    if not os.path.exists(local_dir):\n",
    "        os.mkdir(local_dir)\n",
    "        \n",
    "    complete_path = os.path.join(local_dir, local_file)\n",
    "    if os.path.exists(complete_path):\n",
    "        print('File %s exists, not downloading'%complete_path)\n",
    "    else:\n",
    "        print('Downloading %s'%target_url)\n",
    "        urlretrieve(target_url, complete_path)\n",
    "        print('File downloaded')\n",
    "    return complete_path\n",
    "\n",
    "url = 'https://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt'\n",
    "local_file = maybe_download(url, 'Shakespeare', 'shakespeare_input.txt')\n",
    "\n",
    "with open(local_file, 'r') as f:\n",
    "    data = f.read()\n",
    "    \n",
    "chars = list(set(data))\n",
    "print('Number of unique characters are', len(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create lookup and reverse lookup for vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_ix = {c:ix for ix, c in enumerate(chars)}\n",
    "ix_to_char = {ix:c for ix, c in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets define some parameters for the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "seq_length = 50 #Number of previous characters (timesteps) to train on\n",
    "\n",
    "vocab_size = len(chars)\n",
    "\n",
    "hidden_dim = 500\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "num_layers = 2\n",
    "\n",
    "generate_length = 500\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Define input and output numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def build_input(input_text):\n",
    "    batches = len(input_text) // seq_length\n",
    "    X = np.zeros([batches, seq_length, vocab_size])\n",
    "    Y = np.zeros([batches, seq_length, vocab_size])\n",
    "    \n",
    "    for i in range(0, batches):\n",
    "        x_slice_text = input_text[(i * seq_length) : (i + 1) * seq_length]\n",
    "        y_slice_text = input_text[(i * seq_length + 1) : ((i + 1) * seq_length) + 1]\n",
    "        x_slice_text_ix = [char_to_ix[t] for t in x_slice_text]\n",
    "        y_slice_text_ix = [char_to_ix[t] for t in y_slice_text]\n",
    "        for j in range(seq_length):\n",
    "            X[i, j, x_slice_text_ix[j]] = 1\n",
    "            Y[i, j, y_slice_text_ix[j]] = 1            \n",
    "            \n",
    "    return X, Y\n",
    "\n",
    "X, Y = build_input(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method to generate the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, length, vocab_size, ix_to_char):\n",
    "    ix = np.random.randint(vocab_size)\n",
    "    y_char = [ix_to_char[ix]]\n",
    "    X = np.zeros((1, length, vocab_size))\n",
    "    for i in range(length):\n",
    "        # appending the last predicted character to sequence\n",
    "        X[0, i, :][ix] = 1\n",
    "        print(ix_to_char[ix], end=\"\")\n",
    "        ix = np.argmax(model.predict(X[:, :i+1, :])[0], 1)\n",
    "        y_char.append(ix_to_char[ix])\n",
    "    return ('').join(y_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now lets define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import LSTM, SimpleRNN\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(hidden_dim, input_shape=(None, vocab_size), return_sequences=True))\n",
    "for i in range(num_layers - 1):\n",
    "  model.add(LSTM(hidden_dim, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(vocab_size)))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 0\n",
      "\n",
      "Epoch 1/1\n",
      "55550/91466 [=================>............] - ETA: 18:49 - loss: 2.0615"
     ]
    }
   ],
   "source": [
    "num_epoch = 0\n",
    "\n",
    "while num_epoch < 10:\n",
    "    print('\\n\\nEpoch: {}\\n'.format(num_epoch))\n",
    "    model.fit(X, Y, batch_size = batch_size, verbose = 1, epochs = 1)\n",
    "    num_epoch += 1\n",
    "    generate_text(model, generate_length, vocab_size, ix_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
