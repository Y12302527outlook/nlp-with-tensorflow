{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Introduction\n",
    "\n",
    "Implementation of RNNs for sentence generation using Keras. The reference taken is [this](https://chunml.github.io/ChunML.github.io/project/Creating-Text-Generator-Using-Recurrent-Neural-Network/) website. The dataset use is Andrej Karpathy's [page](https://cs.stanford.edu/people/karpathy/char-rnn/)\n",
    "\n",
    "First we will download the text if required and load the data as characters in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Shakespeare/shakespeare_input.txt exists, not downloading\n",
      "Number of unique characters are 67\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "def maybe_download(target_url, local_dir, local_file):\n",
    "    if not os.path.exists(local_dir):\n",
    "        os.mkdir(local_dir)\n",
    "        \n",
    "    complete_path = os.path.join(local_dir, local_file)\n",
    "    if os.path.exists(complete_path):\n",
    "        print('File %s exists, not downloading'%complete_path)\n",
    "    else:\n",
    "        print('Downloading %s'%target_url)\n",
    "        urlretrieve(target_url, complete_path)\n",
    "        print('File downloaded')\n",
    "    return complete_path\n",
    "\n",
    "url = 'https://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt'\n",
    "local_file = maybe_download(url, 'Shakespeare', 'shakespeare_input.txt')\n",
    "\n",
    "with open(local_file, 'r') as f:\n",
    "    data = f.read()\n",
    "    \n",
    "chars = list(set(data))\n",
    "print('Number of unique characters are', len(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create lookup and reverse lookup for vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_ix = {c:ix for ix, c in enumerate(chars)}\n",
    "ix_to_char = {ix:c for ix, c in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets define some parameters for the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "seq_length = 50 #Number of previous characters (timesteps) to train on\n",
    "\n",
    "vocab_size = len(chars)\n",
    "\n",
    "hidden_dim = 500\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "num_layers = 2\n",
    "\n",
    "generate_length = 500\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Define input and output numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def build_input(input_text):\n",
    "    batches = len(input_text) // seq_length\n",
    "    X = np.zeros([batches, seq_length, vocab_size])\n",
    "    Y = np.zeros([batches, seq_length, vocab_size])\n",
    "    \n",
    "    for i in range(0, batches):\n",
    "        x_slice_text = input_text[(i * seq_length) : (i + 1) * seq_length]\n",
    "        y_slice_text = input_text[(i * seq_length + 1) : ((i + 1) * seq_length) + 1]\n",
    "        x_slice_text_ix = [char_to_ix[t] for t in x_slice_text]\n",
    "        y_slice_text_ix = [char_to_ix[t] for t in y_slice_text]\n",
    "        for j in range(seq_length):\n",
    "            X[i, j, x_slice_text_ix[j]] = 1\n",
    "            Y[i, j, y_slice_text_ix[j]] = 1            \n",
    "            \n",
    "    return X, Y\n",
    "\n",
    "X, Y = build_input(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method to generate the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, length, vocab_size, ix_to_char):\n",
    "    ix = [np.random.randint(vocab_size)]\n",
    "    y_char = [ix_to_char[ix[-1]]]\n",
    "    X = np.zeros((1, length, vocab_size))\n",
    "    for i in range(length):\n",
    "        # appending the last predicted character to sequence\n",
    "        X[0, i, :][ix[-1]] = 1\n",
    "        print(ix_to_char[ix[-1]], end=\"\")\n",
    "        ix = np.argmax(model.predict(X[:, :i+1, :])[0], 1)\n",
    "        y_char.append(ix_to_char[ix[-1]])\n",
    "    return ('').join(y_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now lets define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import LSTM, SimpleRNN\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(hidden_dim, input_shape=(None, vocab_size), return_sequences=True))\n",
    "for i in range(num_layers - 1):\n",
    "  model.add(LSTM(hidden_dim, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(vocab_size)))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 0\n",
      "\n",
      "Epoch 1/1\n",
      "91466/91466 [==============================] - 2605s 28ms/step - loss: 1.8875\n",
      "[ the surget of the surget of the surget of the surget of the surget of the surget of the surget of the surget of the surget of the surget of the surget of the surget of the surget of the surget of the surget of the surget of the surget of the surget of the surget of the surget of the surget of the surget of the surget of the surget of the surget of the surget of the surget of the surget of the surget of the surget of the surget of the surget of the surget of the surget of the surget of the surg\n",
      "\n",
      "Epoch: 1\n",
      "\n",
      "Epoch 1/1\n",
      "91466/91466 [==============================] - 2603s 28ms/step - loss: 1.4265\n",
      "$ the state of the state of the state\n",
      "The state of the state of the state of the state of the state\n",
      "The state of the state of the state of the state of the state\n",
      "The state of the state of the state of the state of the state\n",
      "The state of the state of the state of the state of the state\n",
      "The state of the state of the state of the state of the state\n",
      "The state of the state of the state of the state of the state\n",
      "The state of the state of the state of the state of the state\n",
      "The state of the state of th\n",
      "\n",
      "Epoch: 2\n",
      "\n",
      "Epoch 1/1\n",
      "91466/91466 [==============================] - 2600s 28ms/step - loss: 1.3455\n",
      "will not be the laws of the control of the contract of the streets,\n",
      "The sea of the controlly of the streets,\n",
      "The sea of the controlly of the streets,\n",
      "The sea and the most strange the prince of the controlly.\n",
      "\n",
      "CASSIO:\n",
      "I have the laws of the control of the contract.\n",
      "\n",
      "PRINCE HENRY:\n",
      "The senate of the consequence of the streets,\n",
      "The sea of the controlly of the streets,\n",
      "The sea of the controlly of the streets,\n",
      "The sea and the most strange the prince of the controlly.\n",
      "\n",
      "CASSIO:\n",
      "I have the laws of the co\n",
      "\n",
      "Epoch: 3\n",
      "\n",
      "Epoch 1/1\n",
      "91466/91466 [==============================] - 2598s 28ms/step - loss: 1.3023\n",
      "ou art a stay.\n",
      "\n",
      "LADY MACBETH:\n",
      "I have seen to hear the seal of the commonwealth.\n",
      "\n",
      "LEONTES:\n",
      "I would the king have seen to hear the seal of the commonwealth.\n",
      "\n",
      "LEONTES:\n",
      "I would the king have seen to hear the seal of the commonwealth.\n",
      "\n",
      "LEONTES:\n",
      "I would the king have seen to hear the seal of the commonwealth.\n",
      "\n",
      "LEONTES:\n",
      "I would the king have seen to hear the seal of the commonwealth.\n",
      "\n",
      "LEONTES:\n",
      "I would the king have seen to hear the seal of the commonwealth.\n",
      "\n",
      "LEONTES:\n",
      "I would the king have seen to hear \n",
      "\n",
      "Epoch: 4\n",
      "\n",
      "Epoch 1/1\n",
      "91466/91466 [==============================] - 2600s 28ms/step - loss: 1.2716\n",
      "ll the state of the state,\n",
      "And therefore the sun shall be so best to seek the state,\n",
      "And therefore the sun shall be so best to seek the state,\n",
      "And therefore the sun shall be so best to seek the state,\n",
      "And therefore the sun shall be so best to seek the state,\n",
      "And therefore the sun shall be so best to seek the state,\n",
      "And therefore the sun shall be so best to seek the state,\n",
      "And therefore the sun shall be so best to seek the state,\n",
      "And therefore the sun shall be so best to seek the state,\n",
      "And there\n",
      "\n",
      "Epoch: 5\n",
      "\n",
      "Epoch 1/1\n",
      "91466/91466 [==============================] - 2594s 28ms/step - loss: 1.2465\n",
      "LET:\n",
      "Why, then, the king has made me still the law\n",
      "With the start of the seas and starts of the start,\n",
      "The seal of the start of the seas and starts of the\n",
      "court, the sun of the court are but a man,\n",
      "That shall be stay'd and stay'd and stay'd and stay'd and stay'd\n",
      "The starting of the seas and starts of the start,\n",
      "The starting of the seas and starts of the start,\n",
      "The starting of the seas and starts of the start,\n",
      "The starting of the seas and starts of the start,\n",
      "The starting of the seas and starts o\n",
      "\n",
      "Epoch: 6\n",
      "\n",
      "Epoch 1/1\n",
      "91466/91466 [==============================] - 2597s 28ms/step - loss: 1.2246\n",
      "ge of the world with me,\n",
      "And she will see thee from the world and so much shall be so\n",
      "That may be so become a prince and my soul is dead.\n",
      "\n",
      "LAUNCE:\n",
      "Why, then, the king is dead, and there is no matter in the world of the world.\n",
      "\n",
      "LEONATO:\n",
      "The man is my lord.\n",
      "\n",
      "LADY MACBETH:\n",
      "The sun with me to seek the wine of thee.\n",
      "\n",
      "LUCETTA:\n",
      "Ay, my lord.\n",
      "\n",
      "LAUNCE:\n",
      "Why, then, the king is dead, and there is no matter in the world of the world.\n",
      "\n",
      "LEONATO:\n",
      "The man is my lord.\n",
      "\n",
      "LADY MACBETH:\n",
      "The sun with me to seek the win\n",
      "\n",
      "Epoch: 7\n",
      "\n",
      "Epoch 1/1\n",
      "91466/91466 [==============================] - 2598s 28ms/step - loss: 1.2042\n",
      "be so bold to see thee well.\n",
      "\n",
      "GLOUCESTER:\n",
      "I do not know the chain of the court of the world,\n",
      "The sea is dead, and there is some of the world and the streets.\n",
      "\n",
      "SIR TOBY BELCH:\n",
      "What say you?\n",
      "\n",
      "SIR TOBY BELCH:\n",
      "What say you?\n",
      "\n",
      "SIR TOBY BELCH:\n",
      "What say you?\n",
      "\n",
      "SIR TOBY BELCH:\n",
      "What say you?\n",
      "\n",
      "SIR TOBY BELCH:\n",
      "What say you?\n",
      "\n",
      "SIR TOBY BELCH:\n",
      "What say you?\n",
      "\n",
      "SIR TOBY BELCH:\n",
      "What say you?\n",
      "\n",
      "SIR TOBY BELCH:\n",
      "What say you?\n",
      "\n",
      "SIR TOBY BELCH:\n",
      "What say you?\n",
      "\n",
      "SIR TOBY BELCH:\n",
      "What say you?\n",
      "\n",
      "SIR TOBY BELCH:\n",
      "What say you?\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 8\n",
      "\n",
      "Epoch 1/1\n",
      "91466/91466 [==============================] - 2599s 28ms/step - loss: 1.1856\n",
      "DESDEMONA:\n",
      "I will tell you that I am a man of the world.\n",
      "\n",
      "KING HENRY V:\n",
      "I will not speak to thee.\n",
      "\n",
      "CASSIUS:\n",
      "I will not speak to thee.\n",
      "\n",
      "CASSIUS:\n",
      "I will not speak to thee.\n",
      "\n",
      "CASSIUS:\n",
      "I will not speak to thee.\n",
      "\n",
      "CASSIUS:\n",
      "I will not speak to thee.\n",
      "\n",
      "CASSIUS:\n",
      "I will not speak to thee.\n",
      "\n",
      "CASSIUS:\n",
      "I will not speak to thee.\n",
      "\n",
      "CASSIUS:\n",
      "I will not speak to thee.\n",
      "\n",
      "CASSIUS:\n",
      "I will not speak to thee.\n",
      "\n",
      "CASSIUS:\n",
      "I will not speak to thee.\n",
      "\n",
      "CASSIUS:\n",
      "I will not speak to thee.\n",
      "\n",
      "CASSIUS:\n",
      "I will not speak to thee.\n",
      "\n",
      "CASSI\n",
      "\n",
      "Epoch: 9\n",
      "\n",
      "Epoch 1/1\n",
      "91466/91466 [==============================] - 2641s 29ms/step - loss: 1.1675\n",
      ":\n",
      "I will not speak with you.\n",
      "\n",
      "FALSTAFF:\n",
      "What is the matter?\n",
      "\n",
      "COSTARD:\n",
      "I will not be a maid, and the whole world will be a stranger than the world\n",
      "And the wind of the world will be a stranger\n",
      "Than the sea-side of the world will be a stranger\n",
      "Than the sea-side of the world will be a stranger\n",
      "Than the sea-side of the world will be a stranger\n",
      "Than the sea-side of the world will be a stranger\n",
      "Than the sea-side of the world will be a stranger\n",
      "Than the sea-side of the world will be a stranger\n",
      "Than the "
     ]
    }
   ],
   "source": [
    "num_epoch = 0\n",
    "\n",
    "while num_epoch < 200:\n",
    "    print('\\n\\nEpoch: {}\\n'.format(num_epoch))\n",
    "    model.fit(X, Y, batch_size = batch_size, verbose = 1, epochs = 1)\n",
    "    num_epoch += 1\n",
    "    generate_text(model, generate_length, vocab_size, ix_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('trained_10_epochs.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
