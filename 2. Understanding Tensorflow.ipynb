{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Sigmoid\n",
    "\n",
    "Following implements $sigmoid(x)\\:=\\: \\frac{1}{1 + e^{-x}}$\n",
    "\n",
    "First create a Graph, which is the recipe for the sequence of operations to perform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#Can create a Graph as follows\n",
    "#graph = tf.Graph()\n",
    "#or\n",
    "#graph = tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create tensors\n",
    "- x: Placeholder, the variables that we will give at runtime\n",
    "- W and b: mutable tensors, they change with time, say in each iteration, variables need an initializer\n",
    "- h: imutable tensor, produces as a result of some operations on other tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of h is (?, 5)\n",
      "h_eval is [[ 0.4517242   0.50546724  0.5095306   0.48777589  0.42552006]]\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(shape = [None, 10], name = 'x', dtype = tf.float32)\n",
    "W = tf.Variable(tf.random_uniform(shape = [10, 5], dtype = tf.float32, minval = -0.1, maxval = 0.1), name = 'W')\n",
    "b = tf.Variable(tf.zeros(shape = [5], dtype = tf.float32), name = 'b')\n",
    "h = tf.nn.sigmoid(tf.matmul(x, W) + b)\n",
    "print('shape of h is', h.shape)\n",
    "init = tf.global_variables_initializer()\n",
    "np.random.seed(1234)\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    h_eval = session.run(h, feed_dict = {x: np.random.rand(1, 10)})\n",
    "\n",
    "print('h_eval is', h_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Placeholders can also be replaced by preloaded constants where the values do not depend on runtime as follows, note that we dont need ``feed_dict`` in this case as the input is constant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape if h is (1, 5)\n",
      "h_eval is [[ 0.57311052  0.63654232  0.59043294  0.4804599   0.230312  ]]\n"
     ]
    }
   ],
   "source": [
    "c = tf.constant(value = [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]], dtype= tf.float32)\n",
    "W_c = tf.Variable(tf.random_uniform(shape = [10, 5], dtype = tf.float32, minval = -0.1, maxval = 0.1), name = 'W')\n",
    "b_c = tf.Variable(tf.zeros(shape = [5], dtype = tf.float32), name = 'b')\n",
    "h_c = tf.nn.sigmoid(tf.matmul(c, W) + b)\n",
    "print('shape if h is', h_c.shape)\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    h_eval = session.run(h_c)\n",
    "\n",
    "print('h_eval is', h_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building pipeline\n",
    "\n",
    "We will now source the data from 3 text files files each containing 5 lines with each line being 10 comma seperated float values. First, we will create the files if they don't already exist with sample data and then build a pipeline to read the data from files all the way to loading them into tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_sample_if_not_exist(filename, numrows = 5, numcols = 10, seed = None):\n",
    "    import numpy as np\n",
    "    import os\n",
    "    if os.path.exists(filename):\n",
    "        print('File', filename, 'exists, not creating one')\n",
    "    else:\n",
    "        print('Creating', filename)\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "    \n",
    "        with open(filename, 'w') as f:\n",
    "            for _ in range(numrows):\n",
    "                f.write(\",\".join(np.random.rand(numcols).astype(str).tolist()) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File text1.txt exists, not creating one\n",
      "File text2.txt exists, not creating one\n",
      "File text3.txt exists, not creating one\n"
     ]
    }
   ],
   "source": [
    "filenames = ['text%d.txt'%i for i in range(1, 4)]\n",
    "for filename in filenames:\n",
    "    create_sample_if_not_exist(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a string input producer that will peduce tensors holding the string values held in filenames. We shuffle the input and bound the queue to size 3.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename_queue = tf.train.string_input_producer(filenames, capacity = 3, shuffle = True, name = 'str_inp_prod')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the queue is created, we will initialize a ``TextLineReader`` which will read a line from the text. The read operation returns the values as a key and a value. The key is the name of the file and the value is one line from this file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = tf.TextLineReader()\n",
    "key, value = reader.read(filename_queue, name = 'text_line_op')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decode csv to read 10 values (which we expect in our csv file). These would be 10 independent tensors, we want these 10 values to become our feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Incase of bad line\n",
    "default_record = [[-1.0]] * 10\n",
    "\n",
    "col1, col2, col3, col4, col5, col6, col7, col8, col9, col10 = tf.decode_csv(value, record_defaults = default_record)\n",
    "feature = tf.stack([col1, col2, col3, col4, col5, col6, col7, col8, col9, col10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The variable ``features`` above represents one tensor of size (10, ), representing a single training sample from the input files. We need to read multiple records in batches. Following line allows us to read multiple tensors as a batch. The parameters ``batch_size`` is the mini batch size, capacity is the capacity of the queue containing items, the value ``min_after_dequeue`` represents the number of values to retain in the queue after the dequee operation of the elements in batch. The ``num_threads`` decides how many threads will generate data in parallel.\n",
    "\n",
    "The below setup allows tensor flow read batches of data abstracting the source of real data. The source data will be accessed slowly and can be parallelized to retrieve data asynchronously to the whole training process. The tensor below is our input to the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x is expected to be (<batch_size>, <feature_dimension>) and is (3, 10)\n"
     ]
    }
   ],
   "source": [
    "x = tf.train.shuffle_batch([feature], batch_size = 3, capacity = 5, min_after_dequeue = 1, num_threads = 2)\n",
    "print('shape of x is expected to be (<batch_size>, <feature_dimension>) and is', x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Step 0 ================\n",
      "Evaluated data (x)\n",
      "[[ 0.62277657  0.49368265  0.84053773  0.71209699  0.44390899  0.03103486\n",
      "   0.36323977  0.73072177  0.47556657  0.34441698]\n",
      " [ 0.64088041  0.12620533  0.17146526  0.73708647  0.12702939  0.36964989\n",
      "   0.604334    0.10310444  0.80237418  0.94555324]\n",
      " [ 0.97903883  0.88123226  0.62768191  0.93048656  0.72478998  0.7166779\n",
      "   0.04107857  0.43948177  0.28206977  0.33499596]]\n",
      "Evaluated data (h)\n",
      "[[ 0.48805743  0.46939287  0.54430157  0.45877224  0.48046988]\n",
      " [ 0.51092768  0.48397779  0.54694343  0.48503318  0.4861801 ]\n",
      " [ 0.5046888   0.4737516   0.53621471  0.43979105  0.50475836]]\n",
      "================ Step 1 ================\n",
      "Evaluated data (x)\n",
      "[[ 0.32570741  0.19361869  0.45781165  0.92040259  0.87906915  0.25261575\n",
      "   0.34800878  0.18258873  0.90179604  0.70652819]\n",
      " [ 0.59341133  0.3660745   0.3230947   0.87142324  0.21563406  0.73494518\n",
      "   0.36561909  0.8016026   0.78273559  0.7013554 ]\n",
      " [ 0.3231948   0.59048182  0.85389858  0.28706244  0.17306723  0.13402121\n",
      "   0.99465382  0.17949787  0.31754681  0.56829143]]\n",
      "Evaluated data (h)\n",
      "[[ 0.48697037  0.47061738  0.53159738  0.48073608  0.4903852 ]\n",
      " [ 0.49574265  0.47688112  0.55178684  0.45949769  0.50956297]\n",
      " [ 0.50143343  0.47232312  0.53103137  0.46726215  0.46340334]]\n",
      "================ Step 2 ================\n",
      "Evaluated data (x)\n",
      "[[ 0.72665846  0.90008783  0.77916378  0.59915477  0.29112524  0.15139526\n",
      "   0.33517465  0.65755177  0.07334255  0.0550064 ]\n",
      " [ 0.20175569  0.69829637  0.95219541  0.88996327  0.99356735  0.81870353\n",
      "   0.54512215  0.45125407  0.89055717  0.97326481]\n",
      " [ 0.97449511  0.6677869   0.25565329  0.1083115   0.77618074  0.78247797\n",
      "   0.76160389  0.91440314  0.6586228   0.5683676 ]]\n",
      "Evaluated data (h)\n",
      "[[ 0.49644464  0.47374845  0.53706157  0.4459165   0.48925126]\n",
      " [ 0.48343492  0.45424718  0.53271818  0.45316911  0.49440509]\n",
      " [ 0.48057094  0.47894219  0.54285324  0.41744685  0.47918177]]\n",
      "================ Step 3 ================\n",
      "Evaluated data (x)\n",
      "[[ 0.43689317  0.612149    0.91819805  0.62573665  0.70599759  0.14983371\n",
      "   0.74606341  0.831007    0.63372576  0.43830988]\n",
      " [ 0.28525096  0.62491673  0.4780938   0.19567518  0.38231745  0.05387368\n",
      "   0.45164841  0.98200476  0.1239427   0.1193809 ]\n",
      " [ 0.73852307  0.58730364  0.47163254  0.10712682  0.22921857  0.89996517\n",
      "   0.41675353  0.53585166  0.00620852  0.30064172]]\n",
      "Evaluated data (h)\n",
      "[[ 0.47596732  0.46128529  0.54285699  0.44747749  0.47424325]\n",
      " [ 0.47652236  0.47177097  0.53004915  0.44950002  0.49010891]\n",
      " [ 0.50828433  0.4869118   0.52598572  0.43898177  0.49233717]]\n",
      "================ Step 4 ================\n",
      "Evaluated data (x)\n",
      "[[ 0.15257278  0.56840962  0.52822429  0.95142877  0.48035917  0.50255954\n",
      "   0.53687817  0.81920207  0.05711564  0.66942173]\n",
      " [ 0.76711661  0.70811534  0.79686719  0.55776083  0.96583652  0.14715689\n",
      "   0.029647    0.59389347  0.1140657   0.95080984]\n",
      " [ 0.73852307  0.58730364  0.47163254  0.10712682  0.22921857  0.89996517\n",
      "   0.41675353  0.53585166  0.00620852  0.30064172]]\n",
      "Evaluated data (h)\n",
      "[[ 0.49876833  0.45433426  0.54209214  0.4625116   0.50905484]\n",
      " [ 0.49576947  0.453399    0.53825742  0.45036468  0.4776777 ]\n",
      " [ 0.50828433  0.4869118   0.52598572  0.43898177  0.49233717]]\n"
     ]
    }
   ],
   "source": [
    "#Define the sigmoid operation again\n",
    "\n",
    "W = tf.Variable(tf.random_uniform(shape = (10, 5), minval = -0.1, maxval = 0.1, dtype = tf.float32), name = 'W')\n",
    "b = tf.Variable(tf.zeros(shape = (5, )), dtype = tf.float32, name = 'b')\n",
    "h = tf.nn.sigmoid(tf.matmul(x, W) + b)\n",
    "with tf.Session() as session:\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord = coord, sess = session)\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    for step in range(5):\n",
    "        x_eval, h_eval = session.run([x, h])\n",
    "        print('================ Step %d ================'%step)\n",
    "        print('Evaluated data (x)')\n",
    "        print(x_eval)\n",
    "        print('Evaluated data (h)')\n",
    "        print(h_eval)\n",
    "    \n",
    "    #Request all threads to stop processing\n",
    "    coord.request_stop()\n",
    "    #Wait for all threads to gracefully stop, not needed in a notebook environment\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Variables in tensorflow\n",
    "\n",
    "Following code snippet shows difference between ``tf.Variable`` and ``tf.constant``. variable can be assigned new value but a constant cannot be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught Exception,  'Tensor' object has no attribute 'assign'\n",
      "[variable, constant] is [10, 0]\n"
     ]
    }
   ],
   "source": [
    "variable = tf.Variable(0, dtype = tf.int32)\n",
    "constant = tf.constant(0, dtype = tf.int32)\n",
    "variable = tf.assign(variable, variable + 10)\n",
    "try:\n",
    "    tf.assign(constant, constant + 10)\n",
    "except Exception as e:\n",
    "    print('Caught Exception, ', e)\n",
    "    \n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('[variable, constant] is', session.run([variable, constant]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Casting the variable type is as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable.dtype is <dtype: 'int32_ref'> , variable_float32.dtype is <dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "variable_float32 = tf.cast(variable, dtype = tf.float32)\n",
    "print('variable.dtype is', variable.dtype, ', variable_float32.dtype is', variable_float32.dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializers for variables work as follows (note the difference in constant_initialized). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant_initialized [[ 2.  2.]\n",
      " [ 2.  2.]]\n",
      "zero_initialized [[ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "truncated_norm_initialized [[ 0.92175698 -0.06908354  0.98697382]\n",
      " [-0.11461542 -1.72058022 -0.1628073 ]\n",
      " [ 0.4954699  -0.61831605  0.84827471]]\n",
      "mean of truncated_normal is 6.62201e-05\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "constant_initialized = tf.get_variable(initializer = tf.constant_initializer(2),  shape = [2, 2], \n",
    "                                   name = 'const_init', \n",
    "                                   dtype = tf.float32)\n",
    "\n",
    "zero_initialized = tf.Variable(tf.zeros(shape = [3, 3], dtype = tf.float32), name = 'zeros')\n",
    "random_initialized = tf.Variable(tf.random_uniform(shape = [3, 3], dtype = tf.float32), name = 'rand_init')\n",
    "#Truncated normal drops all values 2 std away from mean and re-picks them\n",
    "truncated_norm_initialized = tf.Variable(tf.truncated_normal(shape =[3, 3], dtype = tf.float32), name = 'tn')\n",
    "large_truncated_norm_initialized = tf.Variable(tf.truncated_normal(shape =[5000, 5000], dtype = tf.float32), \n",
    "                                               name = 'large_tn')\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('constant_initialized', session.run(constant_initialized))\n",
    "    print('zero_initialized', session.run(zero_initialized))\n",
    "    print('truncated_norm_initialized', session.run(truncated_norm_initialized))\n",
    "    print('mean of truncated_normal is', session.run(tf.reduce_mean(large_truncated_norm_initialized)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison operations in Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_equal [[False False]\n",
      " [ True False]]\n",
      "is_less [[ True  True]\n",
      " [False False]]\n",
      "greater_or_equal [[False False]\n",
      " [ True  True]]\n",
      "max_valued [[4 3]\n",
      " [3 4]]\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[1, 2], [3, 4]], dtype = tf.int32)\n",
    "y = tf.constant([[4, 3], [3, 2]], dtype = tf.int32)\n",
    "\n",
    "#Element wise equality, x == y\n",
    "is_equal = tf.equal(x, y)\n",
    "\n",
    "#Element wise less than x < y (similarly we have greater)\n",
    "is_less = tf.less(x, y)\n",
    "\n",
    "#Element wise greater than or equal to (similarly we have less_equal)\n",
    "greater_or_equal = tf.greater_equal(x, y)\n",
    "\n",
    "#Conditional select, takes the form (mask, x, y). If element of mask is true, take from x else y\n",
    "\n",
    "max_valued = tf.where(greater_or_equal, x, y)\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('is_equal', session.run(is_equal))\n",
    "    print('is_less', session.run(is_less))\n",
    "    print('greater_or_equal', session.run(greater_or_equal))\n",
    "    print('max_valued', session.run(max_valued))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Following are some mathematical operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_add_y [[5 5]\n",
      " [6 6]]\n",
      "x_mul_y [[10  7]\n",
      " [24 17]]\n",
      "log_x [[ 0.          0.69314718]\n",
      " [ 1.09861231  1.38629436]]\n",
      "all_sum 10\n",
      "sum_all_rows [4 6] , shape is (2,)\n",
      "sum_all_cols [3 7] , shape is (2,)\n",
      "sum_rows_retain_dims [[4 6]] , shape is (1, 2)\n",
      "sum_cols_retain_dims [[3]\n",
      " [7]] , shape is (2, 1)\n",
      "seg_sum [ 6  9 40]\n"
     ]
    }
   ],
   "source": [
    "#Element wise addition\n",
    "x_add_y = tf.add(x, y)\n",
    "\n",
    "#matrix multiplication\n",
    "x_mul_y = tf.matmul(x, y)\n",
    "\n",
    "#Take natural log, log operation requires type to be float16, float32, float64, complex64, complex128\n",
    "log_x = tf.log(tf.cast(x, tf.float32))\n",
    "\n",
    "#sum all elements to give a scalar\n",
    "all_sum = tf.reduce_sum(x)\n",
    "\n",
    "#Sum across a specific dimension\n",
    "sum_all_rows = tf.reduce_sum(x, axis = 0)\n",
    "\n",
    "sum_all_cols = tf.reduce_sum(x, axis = 1)\n",
    "\n",
    "#Sum across a specific dimension retaining the dimensionality\n",
    "sum_rows_retain_dims = tf.reduce_sum(x, axis = 0, keep_dims = True)\n",
    "\n",
    "sum_cols_retain_dims = tf.reduce_sum(x, axis = 1, keep_dims = True)\n",
    "\n",
    "#sum, grouped by segments, segments expected to have same length as the range to sum on\n",
    "range_const = tf.constant(np.arange(1, 11), dtype = tf.int32, name = 'range')\n",
    "segments = tf.constant([0, 0, 0, 1, 1, 2, 2, 2, 2, 2], dtype = tf.int32, name = 'segments')\n",
    "seg_sum = tf.segment_sum(range_const, segments)\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('x_add_y', session.run(x_add_y))\n",
    "    print('x_mul_y', session.run(x_mul_y))\n",
    "    print('log_x', session.run(log_x))\n",
    "    print('all_sum', session.run(all_sum))\n",
    "    print('sum_all_rows', session.run(sum_all_rows), ', shape is', sum_all_rows.shape)\n",
    "    print('sum_all_cols', session.run(sum_all_cols), ', shape is', sum_all_cols.shape)\n",
    "    print('sum_rows_retain_dims', session.run(sum_rows_retain_dims), ', shape is', sum_rows_retain_dims.shape)\n",
    "    print('sum_cols_retain_dims', session.run(sum_cols_retain_dims), ', shape is', sum_cols_retain_dims.shape)\n",
    "    print('seg_sum', session.run(seg_sum))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Scatter Gather operations\n",
    "\n",
    "We cannot slice a tensor as we would slice a numpy array as ``a[10, 12]`` or ``a[:, 1:3]`` etc. Similarly we cannot selectively set the values of a tensor using the index. Scatter gather operations let us set and get values at specified indices respectively of the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input 1D tensor is [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9.]\n",
      "updated 1D tensor is [  0.   1.   2.  99.  99.  99.   6.   7.   8.   9.]\n",
      "scatter_nd_1, [[ 0.  0.  0.]\n",
      " [ 1.  2.  3.]\n",
      " [ 0.  0.  0.]\n",
      " [ 4.  5.  6.]] , dtype, <dtype: 'float32'>\n",
      "scatter_nd_2, [[0 0 0 0]\n",
      " [0 0 0 1]\n",
      " [0 0 2 0]] , dtype, <dtype: 'int32'>\n"
     ]
    }
   ],
   "source": [
    "#1D scatter operation\n",
    "#The variable to be updated\n",
    "ref = tf.Variable(np.arange(10), dtype = tf.float32, name = 'ref')\n",
    "\n",
    "#Index position to update\n",
    "idx = tf.constant([3, 4, 5], dtype = tf.int32, name = 'idx')\n",
    "update_val = tf.constant([99, 99, 99], dtype = tf.float32, name = 'updated_value')\n",
    "updated = tf.scatter_update(ref, idx, update_val)\n",
    "\n",
    "#Scatter multidimensional.\n",
    "# Initialized to 0 (for both int and float) for numeric values and empty for string\n",
    "index = [[1], [3]] #Row number\n",
    "values = [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]\n",
    "scatter_nd_1 = tf.scatter_nd(index, values, shape = [4, 3])\n",
    "\n",
    "#Anothet example by using both dimensions\n",
    "index = [[1, 3], [2, 2]] #Row and column\n",
    "values = [1, 2]\n",
    "scatter_nd_2 = tf.scatter_nd(index, values, shape = [3, 4])\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('input 1D tensor is', session.run(ref))\n",
    "    print('updated 1D tensor is', session.run(updated))\n",
    "    print('scatter_nd_1,', session.run(scatter_nd_1), ', dtype,', scatter_nd_1.dtype)\n",
    "    print('scatter_nd_2,', session.run(scatter_nd_2), ', dtype,', scatter_nd_2.dtype)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, following are the gather operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slice1d, [1 6]\n",
      "slice2d, [[ 4  5  6]\n",
      " [10 11 12]]\n",
      "slice2d_2, [ 6 11]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1 D slice\n",
    "values = tf.constant(np.arange(10), name = '1d')\n",
    "index = [1, 6]\n",
    "slice1d = tf.gather(values, index)\n",
    "\n",
    "#ND slice\n",
    "values = tf.constant([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\n",
    "index = [[1], [3]]  #Rows\n",
    "slice2d = tf.gather_nd(values, index)\n",
    "\n",
    "#ND Slice, [row, col]\n",
    "index = [[1, 2], [3, 1]]  #[Row, col]\n",
    "slice2d_2 = tf.gather_nd(values, index)\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('slice1d,', session.run(slice1d))\n",
    "    print('slice2d,', session.run(slice2d))\n",
    "    print('slice2d_2,', session.run(slice2d_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolution operation.\n",
    "\n",
    "We will use the following image taken from [this][1] wiki page\n",
    "\n",
    "![Image](input_image.png)\n",
    "\n",
    "and apply convolution operation, specifically we will use the kernel\n",
    "\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "-1 & -1 &-1\\\\\n",
    "-1 & 8 &-1\\\\\n",
    "-1 & -1 &-1\\\\\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "for edge detection\n",
    "\n",
    "[1]:https://en.wikipedia.org/wiki/Kernel_(image_processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrBlD7AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11b331cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "image = np.array(Image.open('input_image.png'))\n",
    "\n",
    "#4D image tensor [batch_size, height, width, channels]\n",
    "image_tensor = tf.constant(image.reshape(1, *image.shape), dtype = tf.float32)\n",
    "\n",
    "arr = np.array([[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]])\n",
    "kernel = np.array([arr, arr, arr]).reshape(3, 3, 3, 1)\n",
    "\n",
    "#4D kernel tensor [kernel_height, kernel_width, input_channels, output_channels]\n",
    "kernel_tensor = tf.constant(kernel, tf.float32)\n",
    "\n",
    "#strides = [batch_stride, height_stride, width_stride, channels_stride]\n",
    "#padding, 'SAME' for padding, 'VALID' for no padding\n",
    "conv_op = tf.nn.conv2d(image_tensor, kernel_tensor, strides = [1, 1, 1, 1], padding = 'VALID')\n",
    "\n",
    "with tf.Session() as session:\n",
    "    conv_image = session.run(conv_op)[0]\n",
    "    \n",
    "conv_shape = conv_image.shape\n",
    "conv_image = conv_image.reshape((conv_shape[0], conv_shape[1]))\n",
    "conv_image[conv_image < 0] = 0\n",
    "plt.axis('Off')\n",
    "plt.imshow(conv_image, cmap = 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reusing Variables with Scoping\n",
    "\n",
    "Debugging tensorflow applications can be tedious. One coding practice involves defining scopes for variable. Let us define a function that runs a simple computation and invoke it multiple time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:0 y:0 add:0\n",
      "[5, 2, 12]\n",
      "x_1:0 y_1:0 add_1:0\n",
      "[5, 2, 12]\n",
      "x_2:0 y_2:0 add_2:0\n",
      "[5, 2, 12]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "def simple_computation(w):\n",
    "    x = tf.Variable(5, name = 'x')\n",
    "    y = tf.Variable(2, name = 'y')\n",
    "    z = w * x + y\n",
    "    return x, y, z\n",
    "    \n",
    "with tf.Session() as session:\n",
    "    for _ in range(3):\n",
    "        x, y, z = simple_computation(2)\n",
    "        tf.global_variables_initializer().run()\n",
    "        print(x.name, y.name, z.name)\n",
    "        print(session.run([x, y, z]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As we see above, the results are identical, however the actual tensorflow variable is different each time we invoke ``simple_computation``, by adding variable score we can ensure we dont create redundant pair variables as follows. Note that the add operation however creates a new variable in different scope (TODO: See how to avoid this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var_scope/x:0 var_scope/y:0 var_scope/add:0\n",
      "[5.0, 2.0, 12.0]\n",
      "var_scope/x:0 var_scope/y:0 var_scope_1/add:0\n",
      "[5.0, 2.0, 12.0]\n",
      "var_scope/x:0 var_scope/y:0 var_scope_2/add:0\n",
      "[5.0, 2.0, 12.0]\n",
      "var_scope/x:0 var_scope/y:0 var_scope_3/add:0\n",
      "[5.0, 2.0, 12.0]\n"
     ]
    }
   ],
   "source": [
    "def not_so_simple_computation(w):\n",
    "    x = tf.get_variable('x', initializer = tf.constant(5, dtype = tf.float32))\n",
    "    y = tf.get_variable('y', initializer = tf.constant(2, dtype = tf.float32))\n",
    "    z = w * x + y\n",
    "    return x, y, z\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Session() as session:\n",
    "    #Invoke once, this creates and initializes the variables once\n",
    "    with tf.variable_scope('var_scope'):\n",
    "        x, y, z = not_so_simple_computation(2)\n",
    "        tf.global_variables_initializer().run()\n",
    "        print(x.name, y.name, z.name)\n",
    "        print(session.run([x, y, z]))\n",
    "\n",
    "    #Sub sequent calls reuse the variable in scope, reuse = True is mandatory else we get an error\n",
    "    for _ in range(3):\n",
    "        with tf.variable_scope('var_scope', reuse = True):\n",
    "            x, y, z = not_so_simple_computation(2)\n",
    "            print(x.name, y.name, z.name)\n",
    "            print(session.run([x, y, z]))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mnist data sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Test accuracy = 0.1135\n",
      "Iteration 5000, Test accuracy = 0.9703\n",
      "Iteration 10000, Test accuracy = 0.9769\n",
      "Iteration 15000, Test accuracy = 0.9819\n",
      "Iteration 20000, Test accuracy = 0.9823\n",
      "Iteration 25000, Test accuracy = 0.9833\n",
      "Iteration 30000, Test accuracy = 0.9835\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "#Input and output placeholders\n",
    "batch_size = 32\n",
    "\n",
    "x = tf.placeholder(name = 'x', shape = [None, 784], dtype = tf.float32)\n",
    "y = tf.placeholder(name = 'y', shape = [None, 10], dtype = tf.float32)\n",
    "\n",
    "#Define variables:\n",
    "\n",
    "def define_net_parameters():\n",
    "    with tf.variable_scope('layer1'):\n",
    "        W = tf.get_variable(name = 'W', \n",
    "                        shape = [784, 500], \n",
    "                        dtype = tf.float32, \n",
    "                        initializer= tf.random_normal_initializer(0, 0.01))\n",
    "        b = tf.get_variable(name = 'b', \n",
    "                        shape = [500, ], \n",
    "                        dtype = tf.float32, \n",
    "                        initializer= tf.random_uniform_initializer(0, 0.01))\n",
    "    \n",
    "    with tf.variable_scope('layer2'):\n",
    "        W = tf.get_variable(name = 'W', \n",
    "                        shape = [500, 250], \n",
    "                        dtype = tf.float32, \n",
    "                        initializer= tf.random_normal_initializer(0, 0.01))\n",
    "        b = tf.get_variable(name = 'b', \n",
    "                        shape = [250,], \n",
    "                        dtype = tf.float32, \n",
    "                        initializer= tf.random_uniform_initializer(0, 0.01))\n",
    "        \n",
    "    with tf.variable_scope('output'):\n",
    "        W = tf.get_variable(name = 'W', \n",
    "                        shape = [250, 10], \n",
    "                        dtype = tf.float32, \n",
    "                        initializer= tf.random_normal_initializer(0, 0.01))\n",
    "        b = tf.get_variable(name = 'b', \n",
    "                        shape = [10,], \n",
    "                        dtype = tf.float32, \n",
    "                        initializer= tf.random_uniform_initializer(0, 0.01))\n",
    "        \n",
    "with tf.Session() as session:\n",
    "    define_net_parameters()\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    with tf.variable_scope('layer1', reuse = True):\n",
    "        W, b = tf.get_variable('W'), tf.get_variable('b')\n",
    "        h1 = tf.nn.relu(tf.matmul(x, W) + b)\n",
    "        \n",
    "    with tf.variable_scope('layer2', reuse = True):\n",
    "        W, b = tf.get_variable('W'), tf.get_variable('b')\n",
    "        h2 = tf.nn.relu(tf.matmul(h1, W) + b)\n",
    "        \n",
    "    with tf.variable_scope('output', reuse = True):\n",
    "        W, b = tf.get_variable('W'), tf.get_variable('b')\n",
    "        logits = tf.matmul(h2, W) + b\n",
    "        \n",
    "        \n",
    "    tf_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = y))\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.1).minimize(tf_loss)\n",
    "    \n",
    "    correctness = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correctness, tf.float32))\n",
    "    \n",
    "    for i in range(30001):\n",
    "        batch = mnist.train.next_batch(batch_size)\n",
    "        train_step.run(feed_dict={x: batch[0], y: batch[1]})\n",
    "        if i % 5000 == 0:\n",
    "            print('Iteration %d, Test accuracy ='%i, accuracy.eval(feed_dict={x: mnist.test.images, y: mnist.test.labels}))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
