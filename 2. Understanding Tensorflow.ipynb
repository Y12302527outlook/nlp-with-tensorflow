{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Sigmoid\n",
    "\n",
    "Following implements $sigmoid(x)\\:=\\: \\frac{1}{1 + e^{-x}}$\n",
    "\n",
    "First create a Graph, which is the recipe for the sequence of operations to perform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#Can create a Graph as follows\n",
    "#graph = tf.Graph()\n",
    "#or\n",
    "#graph = tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create tensors\n",
    "- x: Placeholder, the variables that we will give at runtime\n",
    "- W and b: mutable tensors, they change with time, say in each iteration, variables need an initializer\n",
    "- h: imutable tensor, produces as a result of some operations on other tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of h is (?, 5)\n",
      "h_eval is [[ 0.52049112  0.49635988  0.48334688  0.49741691  0.48985034]]\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(shape = [None, 10], name = 'x', dtype = tf.float32)\n",
    "W = tf.Variable(tf.random_uniform(shape = [10, 5], dtype = tf.float32, minval = -0.1, maxval = 0.1), name = 'W')\n",
    "b = tf.Variable(tf.zeros(shape = [5], dtype = tf.float32), name = 'b')\n",
    "h = tf.nn.sigmoid(tf.matmul(x, W) + b)\n",
    "print('shape of h is', h.shape)\n",
    "init = tf.global_variables_initializer()\n",
    "np.random.seed(1234)\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    h_eval = session.run(h, feed_dict = {x: np.random.rand(1, 10)})\n",
    "\n",
    "print('h_eval is', h_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Placeholders can also be replaced by preloaded constants where the values do not depend on runtime as follows, note that we dont need ``feed_dict`` in this case as the input is constant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape if h is (1, 5)\n",
      "h_eval is [[ 0.15896994  0.47830001  0.7322498   0.34032491  0.73249787]]\n"
     ]
    }
   ],
   "source": [
    "c = tf.constant(value = [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]], dtype= tf.float32)\n",
    "W_c = tf.Variable(tf.random_uniform(shape = [10, 5], dtype = tf.float32, minval = -0.1, maxval = 0.1), name = 'W')\n",
    "b_c = tf.Variable(tf.zeros(shape = [5], dtype = tf.float32), name = 'b')\n",
    "h_c = tf.nn.sigmoid(tf.matmul(c, W) + b)\n",
    "print('shape if h is', h_c.shape)\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    h_eval = session.run(h_c)\n",
    "\n",
    "print('h_eval is', h_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building pipeline\n",
    "\n",
    "We will now source the data from 3 text files files each containing 5 lines with each line being 10 comma seperated float values. First, we will create the files if they don't already exist with sample data and then build a pipeline to read the data from files all the way to loading them into tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_sample_if_not_exist(filename, numrows = 5, numcols = 10, seed = None):\n",
    "    import numpy as np\n",
    "    import os\n",
    "    if os.path.exists(filename):\n",
    "        print('File', filename, 'exists, not creating one')\n",
    "    else:\n",
    "        print('Creating', filename)\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "    \n",
    "        with open(filename, 'w') as f:\n",
    "            for _ in range(numrows):\n",
    "                f.write(\",\".join(np.random.rand(numcols).astype(str).tolist()) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File text1.txt exists, not creating one\n",
      "File text2.txt exists, not creating one\n",
      "File text3.txt exists, not creating one\n"
     ]
    }
   ],
   "source": [
    "filenames = ['text%d.txt'%i for i in range(1, 4)]\n",
    "for filename in filenames:\n",
    "    create_sample_if_not_exist(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a string input producer that will peduce tensors holding the string values held in filenames. We shuffle the input and bound the queue to size 3.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename_queue = tf.train.string_input_producer(filenames, capacity = 3, shuffle = True, name = 'str_inp_prod')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the queue is created, we will initialize a ``TextLineReader`` which will read a line from the text. The read operation returns the values as a key and a value. The key is the name of the file and the value is one line from this file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = tf.TextLineReader()\n",
    "key, value = reader.read(filename_queue, name = 'text_line_op')\n",
    "\n",
    "#Incase of bad line\n",
    "default_record = [[-1.0]] * 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decode csv to read 10 values (which we expect in our csv file). These would be 10 independent tensors, we want these 10 values to become our feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col1, col2, col3, col4, col5, col6, col7, col8, col9, col10 = tf.decode_csv(value, record_defaults = default_record)\n",
    "feature = tf.stack([col1, col2, col3, col4, col5, col6, col7, col8, col9, col10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The variable ``features`` above represents one tensor of size (10, ), representing a single training sample from the input files. We need to read multiple records in batches. Following line allows us to read multiple tensors as a batch. The parameters ``batch_size`` is the mini batch size, capacity is the capacity of the queue containing items, the value ``min_after_dequeue`` represents the number of values to retain in the queue after the dequee operation of the elements in batch. The ``num_threads`` decides how many threads will generate data in parallel.\n",
    "\n",
    "The below setup allows tensor flow read batches of data abstracting the source of real data. The source data will be accessed slowly and can be parallelized to retrieve data asynchronously to the whole training process. The tensor below is our input to the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x is expected to be (<batch_size>, <feature_dimension>) and is (3, 10)\n"
     ]
    }
   ],
   "source": [
    "x = tf.train.shuffle_batch([feature], batch_size = 3, capacity = 5, min_after_dequeue = 1, num_threads = 2)\n",
    "print('shape of x is expected to be (<batch_size>, <feature_dimension>) and is', x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Step 0 ================\n",
      "Evaluated data (x)\n",
      "[[ 0.72665846  0.90008783  0.77916378  0.59915477  0.29112524  0.15139526\n",
      "   0.33517465  0.65755177  0.07334255  0.0550064 ]\n",
      " [ 0.32570741  0.19361869  0.45781165  0.92040259  0.87906915  0.25261575\n",
      "   0.34800878  0.18258873  0.90179604  0.70652819]\n",
      " [ 0.00934857  0.90064859  0.97724146  0.55689466  0.08477385  0.33300248\n",
      "   0.72842866  0.14243537  0.55246896  0.27304325]]\n",
      "Evaluated data (h)\n",
      "[[ 0.49464947  0.52583575  0.50423062  0.49881858  0.50255448]\n",
      " [ 0.45626062  0.53781515  0.50380903  0.49873108  0.51279169]\n",
      " [ 0.48437965  0.51267284  0.50956738  0.50125998  0.52063322]]\n",
      "================ Step 1 ================\n",
      "Evaluated data (x)\n",
      "[[ 0.73852307  0.58730364  0.47163254  0.10712682  0.22921857  0.89996517\n",
      "   0.41675353  0.53585166  0.00620852  0.30064172]\n",
      " [ 0.97449511  0.6677869   0.25565329  0.1083115   0.77618074  0.78247797\n",
      "   0.76160389  0.91440314  0.6586228   0.5683676 ]\n",
      " [ 0.28525096  0.62491673  0.4780938   0.19567518  0.38231745  0.05387368\n",
      "   0.45164841  0.98200476  0.1239427   0.1193809 ]]\n",
      "Evaluated data (h)\n",
      "[[ 0.48740679  0.5114727   0.47919163  0.48941806  0.51768267]\n",
      " [ 0.4787634   0.52715385  0.45991099  0.48764285  0.51501095]\n",
      " [ 0.49576637  0.528705    0.4875924   0.49622399  0.4964214 ]]\n",
      "================ Step 2 ================\n",
      "Evaluated data (x)\n",
      "[[ 0.76711661  0.70811534  0.79686719  0.55776083  0.96583652  0.14715689\n",
      "   0.029647    0.59389347  0.1140657   0.95080984]\n",
      " [ 0.3231948   0.59048182  0.85389858  0.28706244  0.17306723  0.13402121\n",
      "   0.99465382  0.17949787  0.31754681  0.56829143]\n",
      " [ 0.20175569  0.69829637  0.95219541  0.88996327  0.99356735  0.81870353\n",
      "   0.54512215  0.45125407  0.89055717  0.97326481]]\n",
      "Evaluated data (h)\n",
      "[[ 0.48079729  0.56729758  0.5202027   0.49912107  0.50821394]\n",
      " [ 0.4938539   0.51394266  0.5016948   0.48978207  0.5077973 ]\n",
      " [ 0.44648626  0.55200016  0.5031116   0.49865288  0.53150308]]\n",
      "================ Step 3 ================\n",
      "Evaluated data (x)\n",
      "[[ 0.15257278  0.56840962  0.52822429  0.95142877  0.48035917  0.50255954\n",
      "   0.53687817  0.81920207  0.05711564  0.66942173]\n",
      " [ 0.62277657  0.49368265  0.84053773  0.71209699  0.44390899  0.03103486\n",
      "   0.36323977  0.73072177  0.47556657  0.34441698]\n",
      " [ 0.59341133  0.3660745   0.3230947   0.87142324  0.21563406  0.73494518\n",
      "   0.36561909  0.8016026   0.78273559  0.7013554 ]]\n",
      "Evaluated data (h)\n",
      "[[ 0.46881101  0.53225631  0.49617651  0.47402939  0.50444293]\n",
      " [ 0.48164308  0.5401004   0.50317633  0.50206321  0.50215185]\n",
      " [ 0.46698663  0.52720034  0.47983578  0.48141068  0.51741242]]\n",
      "================ Step 4 ================\n",
      "Evaluated data (x)\n",
      "[[ 0.32570741  0.19361869  0.45781165  0.92040259  0.87906915  0.25261575\n",
      "   0.34800878  0.18258873  0.90179604  0.70652819]\n",
      " [ 0.43689317  0.612149    0.91819805  0.62573665  0.70599759  0.14983371\n",
      "   0.74606341  0.831007    0.63372576  0.43830988]\n",
      " [ 0.97903883  0.88123226  0.62768191  0.93048656  0.72478998  0.7166779\n",
      "   0.04107857  0.43948177  0.28206977  0.33499596]]\n",
      "Evaluated data (h)\n",
      "[[ 0.45626062  0.53781515  0.50380903  0.49873108  0.51279169]\n",
      " [ 0.4739182   0.54174566  0.49405295  0.5002737   0.50511128]\n",
      " [ 0.46897835  0.53429425  0.50381672  0.49775097  0.51867723]]\n"
     ]
    }
   ],
   "source": [
    "#Define the sigmoid operation again\n",
    "\n",
    "W = tf.Variable(tf.random_uniform(shape = (10, 5), minval = -0.1, maxval = 0.1, dtype = tf.float32), name = 'W')\n",
    "b = tf.Variable(tf.zeros(shape = (5, )), dtype = tf.float32, name = 'b')\n",
    "h = tf.nn.sigmoid(tf.matmul(x, W) + b)\n",
    "with tf.Session() as session:\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord = coord, sess = session)\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    for step in range(5):\n",
    "        x_eval, h_eval = session.run([x, h])\n",
    "        print('================ Step %d ================'%step)\n",
    "        print('Evaluated data (x)')\n",
    "        print(x_eval)\n",
    "        print('Evaluated data (h)')\n",
    "        print(h_eval)\n",
    "    \n",
    "    #Request all threads to stop processing\n",
    "    coord.request_stop()\n",
    "    #Wait for all threads to gracefully stop, not needed in a notebook environment\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
