{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence Classification\n",
    "\n",
    "We will download the corpus from [http://cogcomp.org/Data/QA/QC/](http://cogcomp.org/Data/QA/QC/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requested file train_1000.label exists locally, no download will be performed\n",
      "Requested file TREC_10.label exists locally, no download will be performed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "import shutil\n",
    "\n",
    "url = 'http://cogcomp.org/Data/QA/QC/'\n",
    "train_file_name = 'train_1000.label'\n",
    "test_file_name = 'TREC_10.label'\n",
    "\n",
    "def maybe_download(url, file_name):\n",
    "    if os.path.exists(file_name):\n",
    "        print('Requested file', file_name, 'exists locally, no download will be performed')\n",
    "    else:\n",
    "        file_url = url + file_name\n",
    "        print('Downloading from', file_url)\n",
    "        local_tmp_file, _ = urlretrieve(file_url)\n",
    "        shutil.move(local_tmp_file, file_name)\n",
    "        print('Remote file successfully downloaded')\n",
    "\n",
    "maybe_download(url, train_file_name)\n",
    "maybe_download(url, test_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file has following format\n",
    "``<class>:<subclass> <question>?``\n",
    "\n",
    "The taxonomy of the class and subclass can be found [here](http://cogcomp.org/Data/QA/QC/definition.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_questions(in_file):\n",
    "    #returns \n",
    "    # class: List of class, the total results are same as the number of questions in the file\n",
    "    # sub_class: List of sub class, the total results are same as the number of questions in the file    \n",
    "    # questions: each item in the list of is a list of split of the question by space\n",
    "    # max_question_len: Maximum length of the question after splitting by space\n",
    "    question_class = []\n",
    "    question_subclass = []\n",
    "    splits = []\n",
    "    max_len = 0\n",
    "    with open(in_file, 'r', encoding = 'latin-1') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            head, *tail = line.split(':')\n",
    "            tail = tail[0].lower().split()\n",
    "            question_class.append(head)\n",
    "            question_subclass.append(tail[0])\n",
    "            splits.append(tail[1:])\n",
    "            max_len = max(max_len, len(tail) - 1)\n",
    "    \n",
    "    return question_class, question_subclass, splits, max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max num of words in train question corpus is 32\n",
      "Max num of words in test question corpus is 17\n"
     ]
    }
   ],
   "source": [
    "train_question_class, _ , train_questions, train_max_len =  read_questions(train_file_name)\n",
    "test_question_class, _ , test_questions, test_max_len =  read_questions(test_file_name)\n",
    "print('Max num of words in train question corpus is', train_max_len)\n",
    "print('Max num of words in test question corpus is', test_max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at first few question categories and question splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question category is DESC\n",
      "\tQuestion tokens are ['how', 'did', 'serfdom', 'develop', 'in', 'and', 'then', 'leave', 'russia', '?']\n",
      "Question category is ENTY\n",
      "\tQuestion tokens are ['what', 'films', 'featured', 'the', 'character', 'popeye', 'doyle', '?']\n",
      "Question category is DESC\n",
      "\tQuestion tokens are ['how', 'can', 'i', 'find', 'a', 'list', 'of', 'celebrities', \"'\", 'real', 'names', '?']\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print('Question category is', train_question_class[i])\n",
    "    print('\\tQuestion tokens are', train_questions[i])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will pad the words with a padding string ``PAD`` to ensure all questions have same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of padded questions is 32\n",
      "Sample padded training question is\n",
      "\t ['how', 'did', 'serfdom', 'develop', 'in', 'and', 'then', 'leave', 'russia', '?', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n"
     ]
    }
   ],
   "source": [
    "def pad_questions(unpadded_questions, max_len):\n",
    "    padded_questions = []\n",
    "    for up in unpadded_questions:\n",
    "        q = ['PAD'] * max_len\n",
    "        padded_questions.append(q)\n",
    "        for i, token in enumerate(up):\n",
    "            q[i] = token\n",
    "        \n",
    "    return padded_questions\n",
    "\n",
    "max_len = max(train_max_len, test_max_len)\n",
    "padded_train_set = pad_questions(train_questions, max_len)\n",
    "padded_test_set = pad_questions(test_questions, max_len)\n",
    "\n",
    "print('Length of padded questions is', max_len)\n",
    "print('Sample padded training question is\\n\\t', padded_train_set[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "We will create the following 4 data structures from the padded questions\n",
    "\n",
    "- dictionary: Mapping between (word, word_id) in corpus\n",
    "- reverse_dictionary: Mapping between (word_id, word) in corpus\n",
    "- count: list of tuples of (word, count of word) ordered by the number of occurrances\n",
    "- data: The data where all words in the question are replaced by the id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts(top 5) [('PAD', 34407), ('?', 1454), ('the', 999), ('what', 963), ('is', 587)]\n",
      "Number of unique words in corpus are 3349\n",
      "Sample question(0) is [9, 15, 982, 983, 6, 23, 984, 985, 518, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Reversed sample question(0) is ['how', 'did', 'serfdom', 'develop', 'in', 'and', 'then', 'leave', 'russia', '?', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Unique labels are {'ABBR', 'DESC', 'ENTY', 'HUM', 'LOC', 'NUM'}\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "\n",
    "def prepare_dataset(padded_question_set):\n",
    "    all_words = [token for q in padded_question_set for token in q]\n",
    "    counts = collections.Counter(all_words).most_common()\n",
    "    dictionary = {}\n",
    "    reverse_dictionary = {}\n",
    "    \n",
    "    for i, (word, _) in enumerate(counts):\n",
    "        dictionary[word] = i\n",
    "        reverse_dictionary[i] = word\n",
    "    \n",
    "    data = [[dictionary[w] for w in q] for q in padded_question_set]\n",
    "    return dictionary, reverse_dictionary, data, counts\n",
    "\n",
    "all_questions = list(padded_train_set)\n",
    "all_questions.extend(padded_test_set)\n",
    "dictionary, reverse_dictionary, dataset, counts = prepare_dataset(all_questions)\n",
    "print('counts(top 5)', counts[:5])\n",
    "print('Number of unique words in corpus are', len(counts))\n",
    "print('Sample question(0) is', dataset[0])\n",
    "print('Reversed sample question(0) is', [reverse_dictionary[i] for i in dataset[0]])\n",
    "unique_labels = set(train_question_class)\n",
    "unique_labels.update(test_question_class)\n",
    "print('Unique labels are', unique_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "We will now define a ``BatchGenerator`` which will generate batches of the give data\n",
    "the batch will return two value\n",
    "\n",
    "- an input of dimension (batch_size, max_sent_length, embedding_size), in our case embedding size will be same as the size of the vocabulary size and the value will be one hot encoded vector\n",
    "- The labels which will be one hot encoded vectors of size same as the number of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One hot for \"the\" has element number 2 set\n",
      "shape of input_batch is (16, 32, 3349) , input_labels has shape (16, 6)\n",
      "first label has value [0 1 0 0 0 0]  labels are ['ABBR', 'DESC', 'ENTY', 'HUM', 'LOC', 'NUM']\n",
      "Labels for first batch are [1 2 1 2 0 3 3 3 1 3 5 1 3 3 2 4] first 16 labels are ['DESC', 'ENTY', 'DESC', 'ENTY', 'ABBR', 'HUM', 'HUM', 'HUM', 'DESC', 'HUM', 'NUM', 'DESC', 'HUM', 'HUM', 'ENTY', 'LOC']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class BatchGenerator(object):\n",
    "    \n",
    "    def __init__(self, batch_size, dataset, labels, word2emb, unique_labels):\n",
    "        #\n",
    "        # batch_size: Batch size\n",
    "        # dataset: is the prepared dataset from previous step        \n",
    "        # word2emb: function that generates embedding from given word\n",
    "        #\n",
    "        self.dataset = dataset\n",
    "        self.word2emb = word2emb\n",
    "        self.current_idx = 0\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.unique_labels = list(unique_labels)\n",
    "        None\n",
    "    \n",
    "    \n",
    "    def __shape_question__(self, padded_question):\n",
    "        return [to_one_hot_encoding(w) for w in padded_question]\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_idx = 0\n",
    "    \n",
    "    def generate_batch(self):\n",
    "        batch = []\n",
    "        labels = []\n",
    "        for _ in range(self.batch_size):\n",
    "            batch.append(self.__shape_question__(self.dataset[self.current_idx]))\n",
    "            c_label = [0] * len(self.unique_labels)\n",
    "            c_label[self.unique_labels.index(self.labels[self.current_idx])] = 1\n",
    "            labels.append(c_label)\n",
    "            self.current_idx += 1\n",
    "            self.current_idx %= len(dataset)\n",
    "            \n",
    "        return np.array(batch), np.array(labels)\n",
    "    \n",
    "\n",
    "def to_one_hot_encoding(word):\n",
    "    one_hot = [0] * len(dictionary)\n",
    "    one_hot[word] = 1\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "prepared_train_dataset = dataset[0:1000]\n",
    "prepared_test_dataset = dataset[1000:]\n",
    "the_one_hot = to_one_hot_encoding(dictionary['the'])\n",
    "print('One hot for \"the\" has element number', the_one_hot.index(1), 'set')\n",
    "batch_size = 16\n",
    "\n",
    "train_batch_generator = BatchGenerator(\n",
    "                            batch_size, \n",
    "                            prepared_train_dataset, \n",
    "                            train_question_class, \n",
    "                            to_one_hot_encoding, \n",
    "                            unique_labels)\n",
    "\n",
    "input_batch, input_labels = train_batch_generator.generate_batch()\n",
    "print('shape of input_batch is', input_batch.shape, ', input_labels has shape', input_labels.shape)\n",
    "print('first label has value', input_labels[0], ' labels are', train_batch_generator.unique_labels)\n",
    "print('Labels for first batch are', \n",
    "      np.argmax(input_labels, axis = 1), 'first 16 labels are', train_question_class[0:16])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now that we have the necessary infrastructure in place, let us setup the CNN to and train the network\n",
    "\n",
    "First we will define the input and label placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input and labels are (32, 32, 3349) and (32, 6) respectively\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#Each sentence is made of size max_len with PAD at the end to fill up the blanks. Each word is one hot encoded\n",
    "#This each sentence is max_len X embedding_size\n",
    "#We then have a batch of such matrix\n",
    "embedding_size = len(dictionary) #Since  we use one hot encoded labels, the size is same as vocabulary size\n",
    "num_labels = len(unique_labels)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "batch_size = 32\n",
    "x = tf.placeholder(shape = [batch_size, max_len, embedding_size], dtype = tf.float32, name = 'input')\n",
    "\n",
    "#Output is one hot encoded for the label types\n",
    "y = tf.placeholder(shape = [batch_size, num_labels], dtype = tf.float32, name = 'labels')\n",
    "\n",
    "print('Shape of input and labels are', x.shape, 'and', y.shape, 'respectively')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "We now define 1-D convolution of different size over the input. These convolutions filters are of different size and are applied in parallel to the sentence input. \n",
    "\n",
    "Next an activation function is applied to the result of convolution and max pooling applied giving scalar value per convolution. The data input to convolution layer by default needs to have the dimensions ``[filter_size, input_width, input_channel]`` as the default value of the ``data_format`` input is `NWC`. \n",
    "\n",
    "In our case the number of input channels size if the embedding and input width is the sentence width.\n",
    "\n",
    "The 1-D convolution filter has the dimension ``[filter_size, input_channels, output_channels]`` In out case the ``output_channels`` is 1 and ``input_channels`` is the vocabulary size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolutions produce tensor of dimension (32, 32, 1)\n",
      "Max Pooling produce tensor of dimension (32, 1)\n"
     ]
    }
   ],
   "source": [
    "filter_sizes = [3, 5, 7] \n",
    "\n",
    "kernels = [tf.Variable(\n",
    "                tf.truncated_normal(shape = [f, embedding_size, 1], stddev = 0.02, dtype = tf.float32), \n",
    "                name = 'W' + str(i)) for i, f in enumerate(filter_sizes)]\n",
    "\n",
    "bias = [tf.Variable(\n",
    "                tf.random_normal(shape = [1], mean = 0, stddev = 0.01, dtype = tf.float32), \n",
    "                name = 'b' + str(i)) for i, f in enumerate(filter_sizes)]\n",
    "\n",
    "\n",
    "conv_layer = [ tf.nn.conv1d(x, k, stride = 1, padding = 'SAME') + b for k, b in zip(kernels, bias)]\n",
    "\n",
    "activations = [tf.nn.relu(c) for c in conv_layer]\n",
    "\n",
    "pooling = [tf.reduce_max(a, axis = 1) for a in activations]\n",
    "\n",
    "print('Convolutions produce tensor of dimension', conv_layer[0].shape)\n",
    "print('Max Pooling produce tensor of dimension', pooling[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "Now  we will define a dense layer of size ``[num_filters, num_labels]`` to come up with logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_stack = tf.concat(pooling, axis=1)\n",
    "\n",
    "W_dense = tf.Variable(\n",
    "            tf.truncated_normal(shape = [len(filter_sizes), num_labels], stddev = 0.5, dtype = tf.float32),\n",
    "            name = 'W_dense')\n",
    "\n",
    "b_dense = tf.Variable(\n",
    "            tf.random_normal(shape = [num_labels], mean = 0, stddev = 0.01, dtype = tf.float32),\n",
    "            name = 'b_dense')\n",
    "\n",
    "logits = tf.matmul(conv_stack, W_dense) + b_dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Define the loss and optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = y))\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate=0.01,momentum=0.9).minimize(loss)\n",
    "\n",
    "predictions = tf.argmax(tf.nn.softmax(logits),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Start the training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 , mean training loss 1.76074\n",
      "Epoch 0 , mean (train, test) accuracy (21.25, 27.901785714285715)\n",
      "Epoch 1 , mean training loss 1.70561\n",
      "Epoch 1 , mean (train, test) accuracy (21.979166666666668, 16.741071428571427)\n",
      "Epoch 2 , mean training loss 1.68196\n",
      "Epoch 2 , mean (train, test) accuracy (21.5625, 16.741071428571427)\n",
      "Epoch 3 , mean training loss 1.67022\n",
      "Epoch 3 , mean (train, test) accuracy (24.895833333333332, 16.741071428571427)\n",
      "Epoch 4 , mean training loss 1.66354\n",
      "Epoch 4 , mean (train, test) accuracy (24.166666666666668, 16.741071428571427)\n",
      "Epoch 5 , mean training loss 1.65905\n",
      "Epoch 5 , mean (train, test) accuracy (24.270833333333332, 16.741071428571427)\n",
      "Epoch 6 , mean training loss 1.65434\n",
      "Epoch 6 , mean (train, test) accuracy (25.520833333333332, 16.741071428571427)\n",
      "Epoch 7 , mean training loss 1.64804\n",
      "Epoch 7 , mean (train, test) accuracy (28.125, 16.964285714285715)\n",
      "Epoch 8 , mean training loss 1.64093\n",
      "Epoch 8 , mean (train, test) accuracy (29.6875, 17.1875)\n",
      "Epoch 9 , mean training loss 1.63268\n",
      "Epoch 9 , mean (train, test) accuracy (30.104166666666664, 17.410714285714285)\n",
      "Epoch 10 , mean training loss 1.62322\n",
      "Epoch 10 , mean (train, test) accuracy (30.312499999999996, 17.633928571428573)\n",
      "Epoch 11 , mean training loss 1.61036\n",
      "Epoch 11 , mean (train, test) accuracy (30.312499999999996, 18.080357142857142)\n",
      "Epoch 12 , mean training loss 1.59289\n",
      "Epoch 12 , mean (train, test) accuracy (31.25, 18.080357142857142)\n",
      "Epoch 13 , mean training loss 1.57237\n",
      "Epoch 13 , mean (train, test) accuracy (31.666666666666664, 22.321428571428573)\n",
      "Epoch 14 , mean training loss 1.54914\n",
      "Epoch 14 , mean (train, test) accuracy (33.4375, 31.026785714285715)\n",
      "Epoch 15 , mean training loss 1.52297\n",
      "Epoch 15 , mean (train, test) accuracy (35.416666666666671, 33.035714285714285)\n",
      "Epoch 16 , mean training loss 1.49417\n",
      "Epoch 16 , mean (train, test) accuracy (40.729166666666664, 39.955357142857146)\n",
      "Epoch 17 , mean training loss 1.46264\n",
      "Epoch 17 , mean (train, test) accuracy (44.270833333333329, 42.857142857142854)\n",
      "Epoch 18 , mean training loss 1.42909\n",
      "Epoch 18 , mean (train, test) accuracy (44.895833333333336, 44.866071428571431)\n",
      "Epoch 19 , mean training loss 1.39418\n",
      "Epoch 19 , mean (train, test) accuracy (48.020833333333336, 48.214285714285715)\n",
      "Epoch 20 , mean training loss 1.35902\n",
      "Epoch 20 , mean (train, test) accuracy (52.1875, 50.669642857142861)\n",
      "Epoch 21 , mean training loss 1.32452\n",
      "Epoch 21 , mean (train, test) accuracy (53.333333333333336, 51.5625)\n",
      "Epoch 22 , mean training loss 1.29174\n",
      "Epoch 22 , mean (train, test) accuracy (54.583333333333329, 52.008928571428569)\n",
      "Epoch 23 , mean training loss 1.2599\n",
      "Epoch 23 , mean (train, test) accuracy (55.520833333333329, 52.901785714285708)\n",
      "Epoch 24 , mean training loss 1.22989\n",
      "Epoch 24 , mean (train, test) accuracy (56.666666666666664, 52.232142857142861)\n",
      "Epoch 25 , mean training loss 1.20101\n",
      "Epoch 25 , mean (train, test) accuracy (57.604166666666664, 52.678571428571431)\n",
      "Epoch 26 , mean training loss 1.17306\n",
      "Epoch 26 , mean (train, test) accuracy (59.687500000000007, 53.125)\n",
      "Epoch 27 , mean training loss 1.14549\n",
      "Epoch 27 , mean (train, test) accuracy (61.145833333333336, 54.017857142857139)\n",
      "Epoch 28 , mean training loss 1.11921\n",
      "Epoch 28 , mean (train, test) accuracy (62.5, 54.464285714285708)\n",
      "Epoch 29 , mean training loss 1.09333\n",
      "Epoch 29 , mean (train, test) accuracy (64.375, 54.910714285714292)\n",
      "Epoch 30 , mean training loss 1.06824\n",
      "Epoch 30 , mean (train, test) accuracy (65.0, 56.026785714285708)\n",
      "Epoch 31 , mean training loss 1.0438\n",
      "Epoch 31 , mean (train, test) accuracy (66.041666666666671, 56.25)\n",
      "Epoch 32 , mean training loss 1.0203\n",
      "Epoch 32 , mean (train, test) accuracy (66.458333333333329, 57.8125)\n",
      "Epoch 33 , mean training loss 0.996947\n",
      "Epoch 33 , mean (train, test) accuracy (67.8125, 58.258928571428569)\n",
      "Epoch 34 , mean training loss 0.974397\n",
      "Epoch 34 , mean (train, test) accuracy (68.75, 59.598214285714292)\n",
      "Epoch 35 , mean training loss 0.952663\n",
      "Epoch 35 , mean (train, test) accuracy (70.0, 60.714285714285708)\n",
      "Epoch 36 , mean training loss 0.931477\n",
      "Epoch 36 , mean (train, test) accuracy (70.9375, 61.607142857142861)\n",
      "Epoch 37 , mean training loss 0.910988\n",
      "Epoch 37 , mean (train, test) accuracy (71.458333333333329, 62.053571428571431)\n",
      "Epoch 38 , mean training loss 0.891044\n",
      "Epoch 38 , mean (train, test) accuracy (72.083333333333329, 62.723214285714292)\n",
      "Epoch 39 , mean training loss 0.872009\n",
      "Epoch 39 , mean (train, test) accuracy (72.708333333333329, 63.169642857142861)\n",
      "Epoch 40 , mean training loss 0.853324\n",
      "Epoch 40 , mean (train, test) accuracy (73.020833333333329, 62.723214285714292)\n",
      "Epoch 41 , mean training loss 0.835207\n",
      "Epoch 41 , mean (train, test) accuracy (73.541666666666671, 63.169642857142861)\n",
      "Epoch 42 , mean training loss 0.817913\n",
      "Epoch 42 , mean (train, test) accuracy (74.583333333333329, 63.169642857142861)\n",
      "Epoch 43 , mean training loss 0.800187\n",
      "Epoch 43 , mean (train, test) accuracy (75.104166666666671, 64.732142857142861)\n",
      "Epoch 44 , mean training loss 0.783568\n",
      "Epoch 44 , mean (train, test) accuracy (75.416666666666671, 65.178571428571431)\n",
      "Epoch 45 , mean training loss 0.767471\n",
      "Epoch 45 , mean (train, test) accuracy (75.9375, 65.178571428571431)\n",
      "Epoch 46 , mean training loss 0.751676\n",
      "Epoch 46 , mean (train, test) accuracy (76.875, 64.732142857142861)\n",
      "Epoch 47 , mean training loss 0.736673\n",
      "Epoch 47 , mean (train, test) accuracy (77.291666666666671, 65.625)\n",
      "Epoch 48 , mean training loss 0.7224\n",
      "Epoch 48 , mean (train, test) accuracy (77.916666666666671, 65.625)\n",
      "Epoch 49 , mean training loss 0.708394\n",
      "Epoch 49 , mean (train, test) accuracy (78.4375, 65.848214285714292)\n"
     ]
    }
   ],
   "source": [
    "train_batch_generator = BatchGenerator(\n",
    "                            batch_size, \n",
    "                            prepared_train_dataset, \n",
    "                            train_question_class, \n",
    "                            to_one_hot_encoding, \n",
    "                            unique_labels)\n",
    "\n",
    "test_batch_generator = BatchGenerator(\n",
    "                            batch_size, \n",
    "                            prepared_test_dataset, \n",
    "                            test_question_class, \n",
    "                            to_one_hot_encoding, \n",
    "                            unique_labels)\n",
    "\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "\n",
    "def accuracy(labels, preds):\n",
    "    return np.sum(np.argmax(labels,axis=1)==preds)/labels.shape[0]\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    \n",
    "    for e in range(num_epochs):\n",
    "        total_loss = []\n",
    "        train_accuracy = []\n",
    "        train_batch_generator.reset()\n",
    "        test_batch_generator.reset()\n",
    "        for _ in range((len(prepared_train_dataset)//batch_size)-1):\n",
    "            input_batch, input_labels = train_batch_generator.generate_batch()\n",
    "            feed_dict = {x: input_batch, y: input_labels}\n",
    "            l, _ , pred= session.run([loss, optimizer, predictions], feed_dict)\n",
    "            train_accuracy.append(accuracy(input_labels, pred))\n",
    "            total_loss.append(l)\n",
    "            \n",
    "        print('Epoch', e, ', mean training loss', np.mean(total_loss))\n",
    "        \n",
    "        test_accuracy = []\n",
    "        for _ in range((len(prepared_test_dataset)//batch_size)-1):\n",
    "            input_batch, input_labels = test_batch_generator.generate_batch()\n",
    "            l, pred = session.run([logits, predictions], feed_dict = {x: input_batch})\n",
    "            test_accuracy.append(accuracy(input_labels, pred))\n",
    "            #print(l[0], pred[0], input_labels[0])\n",
    "            \n",
    "        print('Epoch', e, ', mean (train, test) accuracy', \n",
    "              (np.mean(train_accuracy) * 100, np.mean(test_accuracy) * 100))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_batch_generator.reset()\n",
    "#x, y = test_batch_generator.generate_batch()\n",
    "#def mat_2_sent(mat):\n",
    "#    words = [reverse_dictionary[i] for i in np.argmax(mat, axis = 1)]\n",
    "#    return \" \".join(words)\n",
    "\n",
    "#print([mat_2_sent(x[i,:,:]) for i in range(x.shape[0])])\n",
    "#labs = list()\n",
    "#print([unique_labels[i] for i in np.argmax(y, axis = 1)])\n",
    "#print([reverse_dictionary[i] for i in dataset[1000]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Looks like we are overfitting and the accuracy isn't great.\n",
    "\n",
    "Next steps (TODO)\n",
    "\n",
    "- Read the paper for sentence classification and implement as per the paper. The Paper is given [here](https://arxiv.org/pdf/1408.5882.pdf). Using pretrained word vectors rather than one hot encoded words may give better results\n",
    "- Go through [this](http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/) url and re-implement\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
