{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence Classification\n",
    "\n",
    "We will download the corpus from [http://cogcomp.org/Data/QA/QC/](http://cogcomp.org/Data/QA/QC/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from http://cogcomp.org/Data/QA/QC/train_1000.label\n",
      "Remote file successfully downloaded\n",
      "Downloading from http://cogcomp.org/Data/QA/QC/TREC_10.label\n",
      "Remote file successfully downloaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "import shutil\n",
    "\n",
    "url = 'http://cogcomp.org/Data/QA/QC/'\n",
    "train_file_name = 'train_1000.label'\n",
    "test_file_name = 'TREC_10.label'\n",
    "\n",
    "def maybe_download(url, file_name):\n",
    "    if os.path.exists(file_name):\n",
    "        print('Requested file', file_name, 'exists locally, no download will be performed')\n",
    "    else:\n",
    "        file_url = url + file_name\n",
    "        print('Downloading from', file_url)\n",
    "        local_tmp_file, _ = urlretrieve(file_url)\n",
    "        shutil.move(local_tmp_file, file_name)\n",
    "        print('Remote file successfully downloaded')\n",
    "\n",
    "maybe_download(url, train_file_name)\n",
    "maybe_download(url, test_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file has following format\n",
    "``<class>:<subclass> <question>?``\n",
    "\n",
    "The taxonomy of the class and subclass can be found [here](http://cogcomp.org/Data/QA/QC/definition.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_questions(in_file):\n",
    "    #returns \n",
    "    # class: List of class, the total results are same as the number of questions in the file\n",
    "    # sub_class: List of sub class, the total results are same as the number of questions in the file    \n",
    "    # questions: each item in the list of is a list of split of the question by space\n",
    "    # max_question_len: Maximum length of the question after splitting by space\n",
    "    question_class = []\n",
    "    question_subclass = []\n",
    "    splits = []\n",
    "    max_len = 0\n",
    "    with open(in_file, 'r', encoding = 'latin-1') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            head, *tail = line.split(':')\n",
    "            tail = tail[0].lower().split()\n",
    "            question_class.append(head)\n",
    "            question_subclass.append(tail[0])\n",
    "            splits.append(tail[1:])\n",
    "            max_len = max(max_len, len(tail) - 1)\n",
    "    \n",
    "    return question_class, question_subclass, splits, max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max num of words in train question corpus is 32\n",
      "Max num of words in test question corpus is 17\n"
     ]
    }
   ],
   "source": [
    "train_question_class, _ , train_questions, train_max_len =  read_questions(train_file_name)\n",
    "test_question_class, _ , test_questions, test_max_len =  read_questions(test_file_name)\n",
    "print('Max num of words in train question corpus is', train_max_len)\n",
    "print('Max num of words in test question corpus is', test_max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at first few question categories and question splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question category is DESC\n",
      "\tQuestion tokens are ['how', 'did', 'serfdom', 'develop', 'in', 'and', 'then', 'leave', 'russia', '?']\n",
      "Question category is ENTY\n",
      "\tQuestion tokens are ['what', 'films', 'featured', 'the', 'character', 'popeye', 'doyle', '?']\n",
      "Question category is DESC\n",
      "\tQuestion tokens are ['how', 'can', 'i', 'find', 'a', 'list', 'of', 'celebrities', \"'\", 'real', 'names', '?']\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print('Question category is', train_question_class[i])\n",
    "    print('\\tQuestion tokens are', train_questions[i])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will pad the words with a padding string ``PAD`` to ensure all questions have same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of padded questions is 32\n",
      "Sample padded training question is\n",
      "\t ['how', 'did', 'serfdom', 'develop', 'in', 'and', 'then', 'leave', 'russia', '?', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n"
     ]
    }
   ],
   "source": [
    "def pad_questions(unpadded_questions, max_len):\n",
    "    padded_questions = []\n",
    "    for up in unpadded_questions:\n",
    "        q = ['PAD'] * max_len\n",
    "        padded_questions.append(q)\n",
    "        for i, token in enumerate(up):\n",
    "            q[i] = token\n",
    "        \n",
    "    return padded_questions\n",
    "\n",
    "max_len = max(train_max_len, test_max_len)\n",
    "padded_train_set = pad_questions(train_questions, max_len)\n",
    "padded_test_set = pad_questions(test_questions, max_len)\n",
    "\n",
    "print('Length of padded questions is', max_len)\n",
    "print('Sample padded training question is\\n\\t', padded_train_set[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "We will create the following 4 data structures from the padded questions\n",
    "\n",
    "- dictionary: Mapping between (word, word_id) in corpus\n",
    "- reverse_dictionary: Mapping between (word_id, word) in corpus\n",
    "- count: list of tuples of (word, count of word) ordered by the number of occurrances\n",
    "- data: The data where all words in the question are replaced by the id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts(top 5) [('PAD', 34407), ('?', 1454), ('the', 999), ('what', 963), ('is', 587)]\n",
      "Number of unique words in corpus are 3349\n",
      "Sample question(0) is [9, 15, 982, 983, 6, 23, 984, 985, 518, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Reversed sample question(0) is ['how', 'did', 'serfdom', 'develop', 'in', 'and', 'then', 'leave', 'russia', '?', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Unique labels are {'LOC', 'ENTY', 'DESC', 'NUM', 'HUM', 'ABBR'}\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "\n",
    "def prepare_dataset(padded_question_set):\n",
    "    all_words = [token for q in padded_question_set for token in q]\n",
    "    counts = collections.Counter(all_words).most_common()\n",
    "    dictionary = {}\n",
    "    reverse_dictionary = {}\n",
    "    \n",
    "    for i, (word, _) in enumerate(counts):\n",
    "        dictionary[word] = i\n",
    "        reverse_dictionary[i] = word\n",
    "    \n",
    "    train_data = [[dictionary[w] for w in q] for q in padded_question_set]\n",
    "    return dictionary, reverse_dictionary, train_data, counts\n",
    "\n",
    "all_questions = list(padded_train_set)\n",
    "all_questions.extend(padded_test_set)\n",
    "dictionary, reverse_dictionary, dataset, counts = prepare_dataset(all_questions)\n",
    "print('counts(top 5)', counts[:5])\n",
    "print('Number of unique words in corpus are', len(counts))\n",
    "print('Sample question(0) is', dataset[0])\n",
    "print('Reversed sample question(0) is', [reverse_dictionary[i] for i in dataset[0]])\n",
    "unique_labels = set(train_question_class)\n",
    "unique_labels.update(test_question_class)\n",
    "print('Unique labels are', unique_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "We will now define a ``BatchGenerator`` which will generate batches of the give data\n",
    "the batch will return two value\n",
    "\n",
    "- an input of dimension (batch_size, max_sent_length, embedding_size), in our case embedding size will be same as the size of the vocabulary size and the value will be one hot encoded vector\n",
    "- The labels which will be one hot encoded vectors of size same as the number of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One hot for \"the\" has element number 2 set\n",
      "shape of input_batch is (16, 32, 3349) , input_labels has shape (16, 6)\n",
      "first label has value [0 0 1 0 0 0]  labels are ['LOC', 'ENTY', 'DESC', 'NUM', 'HUM', 'ABBR']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class BatchGenerator(object):\n",
    "    \n",
    "    def __init__(self, batch_size, dataset, labels, word2emb, unique_labels):\n",
    "        #\n",
    "        # batch_size: Batch size\n",
    "        # dataset: is the prepared dataset from previous step        \n",
    "        # word2emb: function that generates embedding from given word\n",
    "        #\n",
    "        self.dataset = dataset\n",
    "        self.word2emb = word2emb\n",
    "        self.current_idx = 0\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.unique_labels = list(unique_labels)\n",
    "        None\n",
    "    \n",
    "    \n",
    "    def __shape_question__(self, padded_question):\n",
    "        return [to_one_hot_encoding(w) for w in padded_question]\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_idx = 0\n",
    "    \n",
    "    def generate_batch(self):\n",
    "        batch = []\n",
    "        labels = []\n",
    "        for _ in range(self.batch_size):\n",
    "            batch.append(self.__shape_question__(dataset[self.current_idx]))\n",
    "            c_label = [0] * len(self.unique_labels)\n",
    "            c_label[self.unique_labels.index(self.labels[self.current_idx])] = 1\n",
    "            labels.append(c_label)\n",
    "            self.current_idx += self.current_idx % len(dataset)\n",
    "            \n",
    "        return np.array(batch), np.array(labels)\n",
    "    \n",
    "\n",
    "def to_one_hot_encoding(word):\n",
    "    one_hot = [0] * len(dictionary)\n",
    "    one_hot[word] = 1\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "prepared_train_dataset = dataset[0:1000]\n",
    "prepared_test_dataset = dataset[1000:]\n",
    "the_one_hot = to_one_hot_encoding(dictionary['the'])\n",
    "print('One hot for \"the\" has element number', the_one_hot.index(1), 'set')\n",
    "batch_size = 16\n",
    "\n",
    "train_batch_generator = BatchGenerator(\n",
    "                            batch_size, \n",
    "                            prepared_train_dataset, \n",
    "                            train_question_class, \n",
    "                            to_one_hot_encoding, \n",
    "                            unique_labels)\n",
    "\n",
    "input_batch, input_labels = train_batch_generator.generate_batch()\n",
    "print('shape of input_batch is', input_batch.shape, ', input_labels has shape', input_labels.shape)\n",
    "print('first label has value', input_labels[0], ' labels are', train_batch_generator.unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
