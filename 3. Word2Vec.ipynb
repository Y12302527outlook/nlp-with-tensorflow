{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec using Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction\n",
    "\n",
    "Meaning of word is the representation or idea conveyed. Word embeddings are numerical representations of the words to make computers understand natural language. The idea is to have similar words or words used in similar context to be close to each other in higher dimension space.\n",
    "\n",
    "But before we look at using word vectors, let us look at classical NLP approach, Wordnet.\n",
    "\n",
    "##### Wordnet\n",
    "\n",
    "- Wordnet is a lexical database encoding parts of speech and tags relationsships between words including nouns, adjectives, verbs and adverbs. \n",
    "- English Wordnet hosts over 150000 words and over 100000 synonym groups(synsets)\n",
    "- Synset is a set of synonyms\n",
    "- Each Synset has a definition which tells what the synset repesents\n",
    "- Each Synonym in a Synset is called a Lemma.\n",
    "- Synsets form a graph and are associated with another synset with a specific type of relationship\n",
    "- Following are the relationship types\n",
    "    - Hypernym of a synset carry a general, high level meaning of a considered synset. For e.g. Vehicle is a hypernym of synset car. It forms `is-a` relation\n",
    "    - Hyponym of a synset carry a more specific meaning of a synset. Toyota Car is a Hyponym of a car. It forms `is-a` relation\n",
    "    - Holonym are synsets that make up the whole entity of the considered synset. If is a `made-of` relation. For example, Tyre has a holonym cars.\n",
    "    - Meronym are opposite of Holonym, they form a `is-made-of` relation.\n",
    "    \n",
    "Let us look at wordnet in action from nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/amolnayak/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset definitions for word car are\n",
      "\n",
      " a motor vehicle with four wheels; usually propelled by an internal combustion engine\n",
      "\n",
      "- a wheeled vehicle adapted to the rails of railroad\n",
      "\n",
      "- the compartment that is suspended from an airship and that carries personnel and the cargo and the power plant\n",
      "\n",
      "- where passengers ride up and down\n",
      "\n",
      "- a conveyance for passengers or freight on a cable railway\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "word = 'car'\n",
    "car_syns = wn.synsets(word)\n",
    "\n",
    "synset_defs = [car_syn.definition() for car_syn in car_syns]\n",
    "print('Synset definitions for word', word, 'are\\n\\n','\\n\\n- '.join(synset_defs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us get the hypernym and holonym of first synset of the cars we got"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypernym of synset containing car are,\n",
      "\t motor_vehicle\n",
      "\t automotive_vehicle\n",
      "\n",
      "Hyponyms of synset containing car are,\n",
      "\t ambulance\n",
      "\t beach_wagon\n",
      "\t station_wagon\n",
      "\t wagon\n",
      "\t estate_car\n",
      "\t beach_waggon\n",
      "\t station_waggon\n",
      "\t waggon\n",
      "\t bus\n",
      "\t jalopy\n",
      "\t heap\n",
      "\t cab\n",
      "\t hack\n",
      "\t taxi\n",
      "\t taxicab\n",
      "\t compact\n",
      "\t compact_car\n",
      "\t convertible\n",
      "\t coupe\n",
      "\t cruiser\n",
      "\t police_cruiser\n",
      "\t patrol_car\n",
      "\t police_car\n",
      "\t prowl_car\n",
      "\t squad_car\n",
      "\t electric\n",
      "\t electric_automobile\n",
      "\t electric_car\n",
      "\t gas_guzzler\n",
      "\t hardtop\n",
      "\t hatchback\n",
      "\t horseless_carriage\n",
      "\t hot_rod\n",
      "\t hot-rod\n",
      "\t jeep\n",
      "\t landrover\n",
      "\t limousine\n",
      "\t limo\n",
      "\t loaner\n",
      "\t minicar\n",
      "\t minivan\n",
      "\t Model_T\n",
      "\t pace_car\n",
      "\t racer\n",
      "\t race_car\n",
      "\t racing_car\n",
      "\t roadster\n",
      "\t runabout\n",
      "\t two-seater\n",
      "\t sedan\n",
      "\t saloon\n",
      "\t sport_utility\n",
      "\t sport_utility_vehicle\n",
      "\t S.U.V.\n",
      "\t SUV\n",
      "\t sports_car\n",
      "\t sport_car\n",
      "\t Stanley_Steamer\n",
      "\t stock_car\n",
      "\t subcompact\n",
      "\t subcompact_car\n",
      "\t touring_car\n",
      "\t phaeton\n",
      "\t tourer\n",
      "\t used-car\n",
      "\t secondhand_car\n"
     ]
    }
   ],
   "source": [
    "car_syn = car_syns[0]\n",
    "\n",
    "hypernyms = car_syn.hypernyms()\n",
    "hypernym_list = '\\n\\t '.join(['\\n\\t '.join(h.lemma_names()) for h in hypernyms])\n",
    "print('Hypernym of synset containing car are,\\n\\t', hypernym_list)\n",
    "\n",
    "hyponyms = car_syn.hyponyms()\n",
    "hyponyms_list = '\\n\\t '.join(['\\n\\t '.join(h.lemma_names()) for h in hyponyms])\n",
    "print('\\nHyponyms of synset containing car are,\\n\\t', hyponyms_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see above, hypernyms are more general than the word `car` and the hyponyms are specyfic types of cars (most of them).\n",
    "\n",
    "Let us look at Holonyms and Meronyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Holonyms found\n",
      "Meronyms are\n",
      "\t accelerator\n",
      "\t accelerator_pedal\n",
      "\t gas_pedal\n",
      "\t gas\n",
      "\t throttle\n",
      "\t gun\n",
      "\t air_bag\n",
      "\t auto_accessory\n",
      "\t automobile_engine\n",
      "\t automobile_horn\n",
      "\t car_horn\n",
      "\t motor_horn\n",
      "\t horn\n",
      "\t hooter\n",
      "\t buffer\n",
      "\t fender\n",
      "\t bumper\n",
      "\t car_door\n",
      "\t car_mirror\n",
      "\t car_seat\n",
      "\t car_window\n",
      "\t fender\n",
      "\t wing\n",
      "\t first_gear\n",
      "\t first\n",
      "\t low_gear\n",
      "\t low\n",
      "\t floorboard\n",
      "\t gasoline_engine\n",
      "\t petrol_engine\n",
      "\t glove_compartment\n",
      "\t grille\n",
      "\t radiator_grille\n",
      "\t high_gear\n",
      "\t high\n",
      "\t hood\n",
      "\t bonnet\n",
      "\t cowl\n",
      "\t cowling\n",
      "\t luggage_compartment\n",
      "\t automobile_trunk\n",
      "\t trunk\n",
      "\t rear_window\n",
      "\t reverse\n",
      "\t reverse_gear\n",
      "\t roof\n",
      "\t running_board\n",
      "\t stabilizer_bar\n",
      "\t anti-sway_bar\n",
      "\t sunroof\n",
      "\t sunshine-roof\n",
      "\t tail_fin\n",
      "\t tailfin\n",
      "\t fin\n",
      "\t third_gear\n",
      "\t third\n",
      "\t window\n"
     ]
    }
   ],
   "source": [
    "holonyms = car_syn.part_holonyms()\n",
    "holonyms = '\\n\\t '.join(['\\n\\t '.join(h.lemma_names()) for h in holonyms])\n",
    "if len(holonyms):\n",
    "    print('Holonyms are\\n\\t', holonyms)\n",
    "else:\n",
    "    print('No Holonyms found')\n",
    "\n",
    "meronyms = '\\n\\t '.join(['\\n\\t '.join(m.lemma_names()) for m in car_syn.part_meronyms()])\n",
    "if len(meronyms):\n",
    "    print('Meronyms are\\n\\t', meronyms)\n",
    "else:\n",
    "    print('No Meronyms found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see above, there are no holonyms of car but a car is composed of a lot of parts and thus we have found a lot of meronyms. \n",
    "\n",
    "If we choose a word from the above meronyms and find its holonyms, we should find car in it as seen below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holonyms of sunroof are\n",
      "\t car\n",
      "\t auto\n",
      "\t automobile\n",
      "\t machine\n",
      "\t motorcar\n",
      "No meronyms for sunroof found\n"
     ]
    }
   ],
   "source": [
    "car_part = 'sunroof'\n",
    "first_synset = wn.synsets(car_part)[0]\n",
    "\n",
    "carpart_holonyms = '\\n\\t '.join(['\\n\\t '.join(h.lemma_names()) for h in first_synset.part_holonyms()])\n",
    "print('Holonyms of', car_part, 'are\\n\\t', carpart_holonyms)\n",
    "\n",
    "carpart_meronyms = '\\n\\t '.join(['\\n\\t '.join(h.lemma_names()) for h in first_synset.part_meronyms()])\n",
    "if len(carpart_meronyms):\n",
    "    print('Meronyms of', car_part, 'are\\n\\t', carpart_meronyms)\n",
    "else:\n",
    "    print('No meronyms for', car_part, 'found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now find similarities between the synsets. (TODO, get more info on similarity metrics). We will use Wu-Palmer similarity to find similarity between all pairs of ``car_syns``\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lemmas in all the synsets are\n",
      "\t car, auto, automobile, machine, motorcar\n",
      "\t car, railcar, railway_car, railroad_car\n",
      "\t car, gondola\n",
      "\t car, elevator_car\n",
      "\t cable_car, car\n",
      "\n",
      "Wu-Palmer similarity matrix constructed is\n",
      " [[ 1.          0.72727273  0.47619048  0.47619048  0.47619048]\n",
      " [ 0.72727273  1.          0.52631579  0.52631579  0.52631579]\n",
      " [ 0.47619048  0.52631579  1.          0.9         0.9       ]\n",
      " [ 0.47619048  0.52631579  0.9         1.          0.9       ]\n",
      " [ 0.47619048  0.52631579  0.9         0.9         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "car_lemmas = '\\n\\t '.join([', '.join(s.lemma_names()) for s in car_syns])\n",
    "print('\\nLemmas in all the synsets are\\n\\t', car_lemmas)\n",
    "sim_mat = np.matrix([[wn.wup_similarity(syn1, syn2) for syn1 in car_syns] for syn2 in car_syns])\n",
    "print('\\nWu-Palmer similarity matrix constructed is\\n', sim_mat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Problems with Wordnet\n",
    "\n",
    "- Misses the nuances of two entities. For example, `want` and `need` have similar meanings with `need` being more assertive\n",
    "- It is subjective as the corpus is created and maintained by a small community\n",
    "- Labor intensive in maintaining Wordnet\n",
    "- Developing Wordnet for other languages is costly\n",
    "\n",
    "\n",
    "#### Vector Reprsentation of words\n",
    "\n",
    "##### One Hot Encoding\n",
    "\n",
    "Consider we have a vocubulary of size V, then each word will be repesented with a vector of size V with the element representing that word in the vocabulary set to 1 and everything else 0.\n",
    "The problem with this approach are \n",
    "- It cannot capture context and similarity between similar words is 0. \n",
    "- The size of vectors become huge as the size of Vocubalary increases.\n",
    "\n",
    "\n",
    "##### TF-IDF\n",
    "\n",
    "It measures the importance of a word in the document. A more frequently occurring word is not necessarily the an important word in the document. TF-IDF takes of this as follows\n",
    "\n",
    "- Term Frequency (TF): The count of a term in the corpus. This term possibly gives more frequently occurring words like `and`, `a`, `the` etc more weight. Formally $TF(w_i, doc)$ = count of $w_i$ in doc / number of words in doc\n",
    "    \n",
    "- Inverse Document frequency (IDF): This term downweights words which are frequent across the corpus. This operation will downweight these words like `and`, `a`, `the` etc. \n",
    "$IDF(w_i)$ = $log$(number of documents / number of documents with word $w_i$)\n",
    "\n",
    "\n",
    "For example, suppose we have the following two sentences (each in its own document) in the corpus\n",
    "\n",
    "- Document 1: This is about cats. Cats are great companions\n",
    "- Document 2: This is about dogs. Dogs are very loyal\n",
    "\n",
    "\n",
    "\n",
    "For document 1,\n",
    "\n",
    "TF-IDF(cats, doc1) = (2 / 8) * log(2/ 1) = 0.075\n",
    " \n",
    "TF-IDF(this, doc1) = (1 / 8) * log(2/ 2) = 0\n",
    "\n",
    "\n",
    "##### Co-occurance Matrix\n",
    "\n",
    "Co-occurance matrix captures the cooccurance of words. If a vocabulary is of size V, the co-occurance matrix is of size $V \\times V$. Unline one hot encoded vectors, we keep a track of the context of the words. The matrix is symmetric across diagonals and only a half of it is enough to convey information.\n",
    "\n",
    "Consider the following two sentences\n",
    "\n",
    "*Jerry and Mary are friends*\n",
    "\n",
    "*Jerry buys flowers for Mary*\n",
    "\n",
    "Following code is an example of building cooccurance matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing get_neighbors for ['jerry', 'and', 'mary', 'are', 'friends']\n",
      "\tget_neighbors(corpus_split_tokens[0], 0) gives ['and']\n",
      "\tget_neighbors(corpus_split_tokens[0], 4) gives ['are']\n",
      "\tget_neighbors(corpus_split_tokens[0], 2) gives ['and', 'are']\n",
      "\n",
      "Vocabulary is ['buys', 'mary', 'for', 'friends', 'and', 'flowers', 'are', 'jerry']\n",
      "Coccurance Matrix is\n",
      " [[ 0.  0.  0.  0.  0.  1.  0.  1.]\n",
      " [ 0.  0.  1.  0.  1.  0.  1.  0.]\n",
      " [ 0.  1.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  1.]\n",
      " [ 1.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  1.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  1.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "corpus = ['Jerry and Mary are friends', 'Jerry buys flowers for Mary']\n",
    "\n",
    "def get_neighbors(sentence, center_word_index):\n",
    "    #Sentence as splits of words\n",
    "    if center_word_index == 0:\n",
    "        return [sentence[1]]\n",
    "    elif center_word_index == len(sentence) - 1:\n",
    "        return [sentence[center_word_index - 1]]\n",
    "    else:\n",
    "        return [sentence[center_word_index - 1], sentence[center_word_index + 1]]\n",
    "\n",
    "split_tokens_sample = corpus[0].lower().split(' ')\n",
    "print('\\nTesting get_neighbors for', split_tokens_sample)\n",
    "print('\\tget_neighbors(corpus_split_tokens[0], 0) gives', get_neighbors(split_tokens_sample, 0))\n",
    "print('\\tget_neighbors(corpus_split_tokens[0], 4) gives', get_neighbors(split_tokens_sample, 4))\n",
    "print('\\tget_neighbors(corpus_split_tokens[0], 2) gives', get_neighbors(split_tokens_sample, 2))\n",
    "\n",
    "def compute_coccurance(corpus):\n",
    "    corpus_split_tokens = [s.lower().split(' ') for s in corpus]\n",
    "    vocabulary = list(set(itertools.chain.from_iterable(corpus_split_tokens)))\n",
    "    print('Vocabulary is', vocabulary)\n",
    "\n",
    "    co_matrix = np.zeros((len(vocabulary), len(vocabulary)))\n",
    "    \n",
    "    for split_sentence in corpus_split_tokens:\n",
    "        for center_word in range(len(split_sentence)):\n",
    "            neighbors = get_neighbors(split_sentence, center_word)\n",
    "            cent_word_vocab_index = vocabulary.index(split_sentence[center_word])\n",
    "            for neighbor in neighbors:\n",
    "                neighbor_word_vocab_index = vocabulary.index(neighbor)\n",
    "                co_matrix[cent_word_vocab_index, neighbor_word_vocab_index] += 1\n",
    "                \n",
    "    return co_matrix\n",
    "\n",
    "print()\n",
    "co_matrix = compute_coccurance(corpus)\n",
    "print('Coccurance Matrix is\\n', co_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can see the problem with this approach\n",
    "\n",
    "- Increase in size of vocabulary increases the size of the matrix polynomially\n",
    "- Context windows larger than 1 will result increased complexity of maintaining cooccurance count. One approach will be to weight down words further from the center word.\n",
    "\n",
    "\n",
    "##### Introducing Word2Vec\n",
    "\n",
    "Word2Vec is a distributed word representation learning technique. It has the following advantages\n",
    "\n",
    "- Its representation is not subjective like Wordnet\n",
    "- It doesn't lose context like one hot vector representation does\n",
    "- It's vector size doesn't depend on the size of the vocabulary unlike one hot encoding or cooccurance matrix.\n",
    "\n",
    "The essence of Word2Vec is to capture the context of the word by looking at the surrounding words. Context means the a finite number of words before and after the center word of interest.\n",
    "\n",
    "Mathematically, the following probability should be high given the center word i.\n",
    "\n",
    "$P(w_{i - m},... w_{i - 1}, w_{i + 1}, ...w_{i + m} \\vert w_i)\\: = \\: \\prod_{j = i - m \\wedge j \\ne i}^{i + m} P(w_j \\vert w_i) $\n",
    "\n",
    "\n",
    "###### Designing the Loss function for learning word embedding\n",
    "\n",
    "We see the above probability if what we try to maximize. To a neural network the loss function $J(\\theta)$ will thus minimize the negative of the above probability.\n",
    "\n",
    "Suppose\n",
    "\n",
    "- N: is the number of words (tokens) in the sentence\n",
    "- m: Window size, that is take m words to the left and m words to the right of the center word\n",
    "\n",
    "The loss function thus will be\n",
    "\n",
    "$J(\\theta)\\: = \\: -\\frac{1}{N - 2m}\\sum_{i = m + 1}^{N - m}\\prod_{j = i - m \\wedge j \\ne i}^{i + m} P(w_j \\vert w_i)$\n",
    "\n",
    "To break it down, we have $\\frac{1}{N - 2m}$ in the term because for a string of length N, we have to start with the $m + 1^{th}$ word and go no more than $N - m^{th}$ word. Thus giving is N - 2m different probabilities. The summation precisly adds up all these probabilities values amnd dividing by the possible values gives the mean.\n",
    "Since the probability is to be maximized and in general while optimizing the weights of a neural network we minimize, we add the negative sign.\n",
    "\n",
    "We dont want to deal with the product in our loss function and thus we take log of the probabilities which converts these products to sum of log probability, thus our loss function becomes\n",
    "\n",
    "$J(\\theta)\\: = \\: -\\frac{1}{N - 2m}\\sum_{i = m + 1}^{N - m}\\sum_{j = i - m \\wedge j \\ne i}^{i + m} log(P(w_j \\vert w_i))$\n",
    "\n",
    "This formulation is called ***Negative log likelyhood***\n",
    "\n",
    "\n",
    "#### The Skipgram Algorithm\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File wikipedia2text-extracted.txt.bz2 already downloaded, using local copy\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlretrieve\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def maybe_download(url, filename):\n",
    "    if os.path.exists(filename):\n",
    "        print('File %s already downloaded, using local copy'%filename)\n",
    "    else:\n",
    "        #Not handling exceptions and missing file errors\n",
    "        print('Downloading file %s from %s'%(filename, url))\n",
    "        local_filename, headers = urlretrieve(url + '/' + filename)\n",
    "        shutil.move(local_filename, filename)\n",
    "    \n",
    "maybe_download('http://www.evanjones.ca/software','wikipedia2text-extracted.txt.bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
