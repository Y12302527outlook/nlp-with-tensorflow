{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec using Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction\n",
    "\n",
    "Meaning of word is the representation or idea conveyed. Word embeddings are numerical representations of the words to make computers understand natural language. The idea is to have similar words or words used in similar context to be close to each other in higher dimension space.\n",
    "\n",
    "But before we look at using word vectors, let us look at classical NLP approach, Wordnet.\n",
    "\n",
    "##### Wordnet\n",
    "\n",
    "- Wordnet is a lexical database encoding parts of speech and tags relationsships between words including nouns, adjectives, verbs and adverbs. \n",
    "- English Wordnet hosts over 150000 words and over 100000 synonym groups(synsets)\n",
    "- Synset is a set of synonyms\n",
    "- Each Synset has a definition which tells what the synset repesents\n",
    "- Each Synonym in a Synset is called a Lemma.\n",
    "- Synsets form a graph and are associated with another synset with a specific type of relationship\n",
    "- Following are the relationship types\n",
    "    - Hypernym of a synset carry a general, high level meaning of a considered synset. For e.g. Vehicle is a hypernym of synset car. It forms `is-a` relation\n",
    "    - Hyponym of a synset carry a more specific meaning of a synset. Toyota Car is a Hyponym of a car. It forms `is-a` relation\n",
    "    - Holonym are synsets that make up the whole entity of the considered synset. If is a `made-of` relation. For example, Tyre has a holonym cars.\n",
    "    - Meronym are opposite of Holonym, they form a `is-made-of` relation.\n",
    "    \n",
    "Let us look at wordnet in action from nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/amolnayak/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset definitions for word car are\n",
      "\n",
      " a motor vehicle with four wheels; usually propelled by an internal combustion engine\n",
      "\n",
      "- a wheeled vehicle adapted to the rails of railroad\n",
      "\n",
      "- the compartment that is suspended from an airship and that carries personnel and the cargo and the power plant\n",
      "\n",
      "- where passengers ride up and down\n",
      "\n",
      "- a conveyance for passengers or freight on a cable railway\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "word = 'car'\n",
    "car_syns = wn.synsets(word)\n",
    "\n",
    "synset_defs = [car_syn.definition() for car_syn in car_syns]\n",
    "print('Synset definitions for word', word, 'are\\n\\n','\\n\\n- '.join(synset_defs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us get the hypernym and holonym of first synset of the cars we got"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypernym of synset containing car are,\n",
      "\t motor_vehicle\n",
      "\t automotive_vehicle\n",
      "\n",
      "Hyponyms of synset containing car are,\n",
      "\t ambulance\n",
      "\t beach_wagon\n",
      "\t station_wagon\n",
      "\t wagon\n",
      "\t estate_car\n",
      "\t beach_waggon\n",
      "\t station_waggon\n",
      "\t waggon\n",
      "\t bus\n",
      "\t jalopy\n",
      "\t heap\n",
      "\t cab\n",
      "\t hack\n",
      "\t taxi\n",
      "\t taxicab\n",
      "\t compact\n",
      "\t compact_car\n",
      "\t convertible\n",
      "\t coupe\n",
      "\t cruiser\n",
      "\t police_cruiser\n",
      "\t patrol_car\n",
      "\t police_car\n",
      "\t prowl_car\n",
      "\t squad_car\n",
      "\t electric\n",
      "\t electric_automobile\n",
      "\t electric_car\n",
      "\t gas_guzzler\n",
      "\t hardtop\n",
      "\t hatchback\n",
      "\t horseless_carriage\n",
      "\t hot_rod\n",
      "\t hot-rod\n",
      "\t jeep\n",
      "\t landrover\n",
      "\t limousine\n",
      "\t limo\n",
      "\t loaner\n",
      "\t minicar\n",
      "\t minivan\n",
      "\t Model_T\n",
      "\t pace_car\n",
      "\t racer\n",
      "\t race_car\n",
      "\t racing_car\n",
      "\t roadster\n",
      "\t runabout\n",
      "\t two-seater\n",
      "\t sedan\n",
      "\t saloon\n",
      "\t sport_utility\n",
      "\t sport_utility_vehicle\n",
      "\t S.U.V.\n",
      "\t SUV\n",
      "\t sports_car\n",
      "\t sport_car\n",
      "\t Stanley_Steamer\n",
      "\t stock_car\n",
      "\t subcompact\n",
      "\t subcompact_car\n",
      "\t touring_car\n",
      "\t phaeton\n",
      "\t tourer\n",
      "\t used-car\n",
      "\t secondhand_car\n"
     ]
    }
   ],
   "source": [
    "car_syn = car_syns[0]\n",
    "\n",
    "hypernyms = car_syn.hypernyms()\n",
    "hypernym_list = '\\n\\t '.join(['\\n\\t '.join(h.lemma_names()) for h in hypernyms])\n",
    "print('Hypernym of synset containing car are,\\n\\t', hypernym_list)\n",
    "\n",
    "hyponyms = car_syn.hyponyms()\n",
    "hyponyms_list = '\\n\\t '.join(['\\n\\t '.join(h.lemma_names()) for h in hyponyms])\n",
    "print('\\nHyponyms of synset containing car are,\\n\\t', hyponyms_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see above, hypernyms are more general than the word `car` and the hyponyms are specyfic types of cars (most of them).\n",
    "\n",
    "Let us look at Holonyms and Meronyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Holonyms found\n",
      "Meronyms are\n",
      "\t accelerator\n",
      "\t accelerator_pedal\n",
      "\t gas_pedal\n",
      "\t gas\n",
      "\t throttle\n",
      "\t gun\n",
      "\t air_bag\n",
      "\t auto_accessory\n",
      "\t automobile_engine\n",
      "\t automobile_horn\n",
      "\t car_horn\n",
      "\t motor_horn\n",
      "\t horn\n",
      "\t hooter\n",
      "\t buffer\n",
      "\t fender\n",
      "\t bumper\n",
      "\t car_door\n",
      "\t car_mirror\n",
      "\t car_seat\n",
      "\t car_window\n",
      "\t fender\n",
      "\t wing\n",
      "\t first_gear\n",
      "\t first\n",
      "\t low_gear\n",
      "\t low\n",
      "\t floorboard\n",
      "\t gasoline_engine\n",
      "\t petrol_engine\n",
      "\t glove_compartment\n",
      "\t grille\n",
      "\t radiator_grille\n",
      "\t high_gear\n",
      "\t high\n",
      "\t hood\n",
      "\t bonnet\n",
      "\t cowl\n",
      "\t cowling\n",
      "\t luggage_compartment\n",
      "\t automobile_trunk\n",
      "\t trunk\n",
      "\t rear_window\n",
      "\t reverse\n",
      "\t reverse_gear\n",
      "\t roof\n",
      "\t running_board\n",
      "\t stabilizer_bar\n",
      "\t anti-sway_bar\n",
      "\t sunroof\n",
      "\t sunshine-roof\n",
      "\t tail_fin\n",
      "\t tailfin\n",
      "\t fin\n",
      "\t third_gear\n",
      "\t third\n",
      "\t window\n"
     ]
    }
   ],
   "source": [
    "holonyms = car_syn.part_holonyms()\n",
    "holonyms = '\\n\\t '.join(['\\n\\t '.join(h.lemma_names()) for h in holonyms])\n",
    "if len(holonyms):\n",
    "    print('Holonyms are\\n\\t', holonyms)\n",
    "else:\n",
    "    print('No Holonyms found')\n",
    "\n",
    "meronyms = '\\n\\t '.join(['\\n\\t '.join(m.lemma_names()) for m in car_syn.part_meronyms()])\n",
    "if len(meronyms):\n",
    "    print('Meronyms are\\n\\t', meronyms)\n",
    "else:\n",
    "    print('No Meronyms found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see above, there are no holonyms of car but a car is composed of a lot of parts and thus we have found a lot of meronyms. \n",
    "\n",
    "If we choose a word from the above meronyms and find its holonyms, we should find car in it as seen below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holonyms of sunroof are\n",
      "\t car\n",
      "\t auto\n",
      "\t automobile\n",
      "\t machine\n",
      "\t motorcar\n",
      "No meronyms for sunroof found\n"
     ]
    }
   ],
   "source": [
    "car_part = 'sunroof'\n",
    "first_synset = wn.synsets(car_part)[0]\n",
    "\n",
    "carpart_holonyms = '\\n\\t '.join(['\\n\\t '.join(h.lemma_names()) for h in first_synset.part_holonyms()])\n",
    "print('Holonyms of', car_part, 'are\\n\\t', carpart_holonyms)\n",
    "\n",
    "carpart_meronyms = '\\n\\t '.join(['\\n\\t '.join(h.lemma_names()) for h in first_synset.part_meronyms()])\n",
    "if len(carpart_meronyms):\n",
    "    print('Meronyms of', car_part, 'are\\n\\t', carpart_meronyms)\n",
    "else:\n",
    "    print('No meronyms for', car_part, 'found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now find similarities between the synsets. (TODO, get more info on similarity metrics). We will use Wu-Palmer similarity to find similarity between all pairs of ``car_syns``\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lemmas in all the synsets are\n",
      "\t car, auto, automobile, machine, motorcar\n",
      "\t car, railcar, railway_car, railroad_car\n",
      "\t car, gondola\n",
      "\t car, elevator_car\n",
      "\t cable_car, car\n",
      "\n",
      "Wu-Palmer similarity matrix constructed is\n",
      " [[ 1.          0.72727273  0.47619048  0.47619048  0.47619048]\n",
      " [ 0.72727273  1.          0.52631579  0.52631579  0.52631579]\n",
      " [ 0.47619048  0.52631579  1.          0.9         0.9       ]\n",
      " [ 0.47619048  0.52631579  0.9         1.          0.9       ]\n",
      " [ 0.47619048  0.52631579  0.9         0.9         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "car_lemmas = '\\n\\t '.join([', '.join(s.lemma_names()) for s in car_syns])\n",
    "print('\\nLemmas in all the synsets are\\n\\t', car_lemmas)\n",
    "sim_mat = np.matrix([[wn.wup_similarity(syn1, syn2) for syn1 in car_syns] for syn2 in car_syns])\n",
    "print('\\nWu-Palmer similarity matrix constructed is\\n', sim_mat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Problems with Wordnet\n",
    "\n",
    "- Misses the nuances of two entities. For example, `want` and `need` have similar meanings with `need` being more assertive\n",
    "- It is subjective as the corpus is created and maintained by a small community\n",
    "- Labor intensive in maintaining Wordnet\n",
    "- Developing Wordnet for other languages is costly\n",
    "\n",
    "\n",
    "#### Vector Reprsentation of words\n",
    "\n",
    "##### One Hot Encoding\n",
    "\n",
    "Consider we have a vocubulary of size V, then each word will be repesented with a vector of size V with the element representing that word in the vocabulary set to 1 and everything else 0.\n",
    "The problem with this approach are \n",
    "- It cannot capture context and similarity between similar words is 0. \n",
    "- The size of vectors become huge as the size of Vocubalary increases.\n",
    "\n",
    "\n",
    "##### TF-IDF\n",
    "\n",
    "It measures the importance of a word in the document. A more frequently occurring word is not necessarily the an important word in the document. TF-IDF takes of this as follows\n",
    "\n",
    "- Term Frequency (TF): The count of a term in the corpus. This term possibly gives more frequently occurring words like `and`, `a`, `the` etc more weight. Formally $TF(w_i, doc)$ = count of $w_i$ in doc / number of words in doc\n",
    "    \n",
    "- Inverse Document frequency (IDF): This term downweights words which are frequent across the corpus. This operation will downweight these words like `and`, `a`, `the` etc. \n",
    "$IDF(w_i)$ = $log$(number of documents / number of documents with word $w_i$)\n",
    "\n",
    "\n",
    "For example, suppose we have the following two sentences (each in its own document) in the corpus\n",
    "\n",
    "- Document 1: This is about cats. Cats are great companions\n",
    "- Document 2: This is about dogs. Dogs are very loyal\n",
    "\n",
    "\n",
    "\n",
    "For document 1,\n",
    "\n",
    "TF-IDF(cats, doc1) = (2 / 8) * log(2/ 1) = 0.075\n",
    " \n",
    "TF-IDF(this, doc1) = (1 / 8) * log(2/ 2) = 0\n",
    "\n",
    "\n",
    "##### Co-occurance Matrix\n",
    "\n",
    "Co-occurance matrix captures the cooccurance of words. If a vocabulary is of size V, the co-occurance matrix is of size $V \\times V$. Unline one hot encoded vectors, we keep a track of the context of the words. The matrix is symmetric across diagonals and only a half of it is enough to convey information.\n",
    "\n",
    "Consider the following two sentences\n",
    "\n",
    "*Jerry and Mary are friends*\n",
    "\n",
    "*Jerry buys flowers for Mary*\n",
    "\n",
    "Following code is an example of building cooccurance matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing get_neighbors for ['jerry', 'and', 'mary', 'are', 'friends']\n",
      "\tget_neighbors(corpus_split_tokens[0], 0) gives ['and']\n",
      "\tget_neighbors(corpus_split_tokens[0], 4) gives ['are']\n",
      "\tget_neighbors(corpus_split_tokens[0], 2) gives ['and', 'are']\n",
      "\n",
      "Vocabulary is ['are', 'jerry', 'flowers', 'and', 'buys', 'friends', 'mary', 'for']\n",
      "Coccurance Matrix is\n",
      " [[ 0.  0.  0.  0.  0.  1.  1.  0.]\n",
      " [ 0.  0.  0.  1.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  1.]\n",
      " [ 0.  1.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  1.  1.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  1.  0.  0.  0.  1.]\n",
      " [ 0.  0.  1.  0.  0.  0.  1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "corpus = ['Jerry and Mary are friends', 'Jerry buys flowers for Mary']\n",
    "\n",
    "def get_neighbors(sentence, center_word_index):\n",
    "    #Sentence as splits of words\n",
    "    if center_word_index == 0:\n",
    "        return [sentence[1]]\n",
    "    elif center_word_index == len(sentence) - 1:\n",
    "        return [sentence[center_word_index - 1]]\n",
    "    else:\n",
    "        return [sentence[center_word_index - 1], sentence[center_word_index + 1]]\n",
    "\n",
    "split_tokens_sample = corpus[0].lower().split(' ')\n",
    "print('\\nTesting get_neighbors for', split_tokens_sample)\n",
    "print('\\tget_neighbors(corpus_split_tokens[0], 0) gives', get_neighbors(split_tokens_sample, 0))\n",
    "print('\\tget_neighbors(corpus_split_tokens[0], 4) gives', get_neighbors(split_tokens_sample, 4))\n",
    "print('\\tget_neighbors(corpus_split_tokens[0], 2) gives', get_neighbors(split_tokens_sample, 2))\n",
    "\n",
    "def compute_coccurance(corpus):\n",
    "    corpus_split_tokens = [s.lower().split(' ') for s in corpus]\n",
    "    vocabulary = list(set(itertools.chain.from_iterable(corpus_split_tokens)))\n",
    "    print('Vocabulary is', vocabulary)\n",
    "\n",
    "    co_matrix = np.zeros((len(vocabulary), len(vocabulary)))\n",
    "    \n",
    "    for split_sentence in corpus_split_tokens:\n",
    "        for center_word in range(len(split_sentence)):\n",
    "            neighbors = get_neighbors(split_sentence, center_word)\n",
    "            cent_word_vocab_index = vocabulary.index(split_sentence[center_word])\n",
    "            for neighbor in neighbors:\n",
    "                neighbor_word_vocab_index = vocabulary.index(neighbor)\n",
    "                co_matrix[cent_word_vocab_index, neighbor_word_vocab_index] += 1\n",
    "                \n",
    "    return co_matrix\n",
    "\n",
    "print()\n",
    "co_matrix = compute_coccurance(corpus)\n",
    "print('Coccurance Matrix is\\n', co_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can see the problem with this approach\n",
    "\n",
    "- Increase in size of vocabulary increases the size of the matrix polynomially\n",
    "- Context windows larger than 1 will result increased complexity of maintaining cooccurance count. One approach will be to weight down words further from the center word.\n",
    "\n",
    "\n",
    "##### Introducing Word2Vec\n",
    "\n",
    "Word2Vec is a distributed word representation learning technique. It has the following advantages\n",
    "\n",
    "- Its representation is not subjective like Wordnet\n",
    "- It doesn't lose context like one hot vector representation does\n",
    "- It's vector size doesn't depend on the size of the vocabulary unlike one hot encoding or cooccurance matrix.\n",
    "\n",
    "The essence of Word2Vec is to capture the context of the word by looking at the surrounding words. Context means the a finite number of words before and after the center word of interest.\n",
    "\n",
    "Mathematically, the following probability should be high given the center word i.\n",
    "\n",
    "$P(w_{i - m},... w_{i - 1}, w_{i + 1}, ...w_{i + m} \\vert w_i)\\: = \\: \\prod_{j = i - m \\wedge j \\ne i}^{i + m} P(w_j \\vert w_i) $\n",
    "\n",
    "\n",
    "###### Designing the Loss function for learning word embedding\n",
    "\n",
    "We see the above probability if what we try to maximize. To a neural network the loss function $J(\\theta)$ will thus minimize the negative of the above probability.\n",
    "\n",
    "Suppose\n",
    "\n",
    "- N: is the number of words (tokens) in the sentence\n",
    "- m: Window size, that is take m words to the left and m words to the right of the center word\n",
    "\n",
    "The loss function thus will be\n",
    "\n",
    "$J(\\theta)\\: = \\: -\\frac{1}{N - 2m}\\sum_{i = m + 1}^{N - m}\\prod_{j = i - m \\wedge j \\ne i}^{i + m} P(w_j \\vert w_i)$\n",
    "\n",
    "To break it down, we have $\\frac{1}{N - 2m}$ in the term because for a string of length N, we have to start with the $m + 1^{th}$ word and go no more than $N - m^{th}$ word. Thus giving is N - 2m different probabilities. The summation precisly adds up all these probabilities values amnd dividing by the possible values gives the mean.\n",
    "Since the probability is to be maximized and in general while optimizing the weights of a neural network we minimize, we add the negative sign.\n",
    "\n",
    "We dont want to deal with the product in our loss function and thus we take log of the probabilities which converts these products to sum of log probability, thus our loss function becomes\n",
    "\n",
    "$J(\\theta)\\: = \\: -\\frac{1}{N - 2m}\\sum_{i = m + 1}^{N - m}\\sum_{j = i - m \\wedge j \\ne i}^{i + m} log(P(w_j \\vert w_i))$\n",
    "\n",
    "This formulation is called ***Negative log likelyhood***\n",
    "\n",
    "\n",
    "#### The Skipgram Algorithm\n",
    "\n",
    "##### Data preparation\n",
    "The first task would be to prepare the input data. Following approach will be used for preparing the input data\n",
    "\n",
    "- For a given word $w_i$ and window size m, the context window including the target word will be of size 2m + 1\n",
    "- If $w_i$ if the current target word, the tuples generated will be [($w_i$, $w_{i - m}$), ...($w_i$, $w_{i - 1}$), ($w_i$, $w_{i + 1}$), ...($w_i$, $w_{i + m}$)]\n",
    "\n",
    "For example, if the sentence is *The dog barked at the mailman*, with window size 1, we will have the following tuples\n",
    "\n",
    "*[(dog, The), (dog, barked), (barked, dog), (barked, at), ..., (the, at), (them mailman)]*\n",
    "\n",
    "These tuples actually are the input and the expected output.\n",
    "\n",
    "##### Network Design\n",
    "\n",
    "- Let the Vocubalary be of size V and our embedding be of dimension D\n",
    "- The embedding layer is of size $V \\times D$, lets call these matrix E\n",
    "- From our input pair *(target, context)*, create a one hot encoded vector of the context word and target word of dimension V. We will call these one hot encoded vectors $x_i$ and $y_i$ respectively\n",
    "- Lookup the embedding for the context word ($x_i$) by doing $x_i \\cdot E$. Since $x_i$ has dimension $1 \\times V$ and E has dimension $V \\times D$, the dot product gives us a vector of size $1 \\times D$, which is the word embedding corresponding to the input context word, let call this embedding vector correponding to the context word, $z_i$\n",
    "- We next find the $logit(x_i)\\:=\\:z_i \\cdot W + b$ where W is the weight matrix of dimension $D \\times V$ and b is the bias vector of dimension $V \\times 1$. The output are logits of dimension $1 \\times V$\n",
    "- We then use softmax layer to generate our prediction $\\hat{y_i}$ where $\\hat{y_i}\\:=\\:softmax(logit(x_i))$\n",
    "- Since we know the context $y_i$, we can back propagate the loss and update the weights and embeddings.\n",
    "\n",
    "\n",
    "The original skipgram model uses two distinct embeddings of dimension $V \\times D$ to represent the word as context and when it is a target word.\n",
    "\n",
    "##### Loss Function\n",
    "\n",
    "We saw earlier that the loss function to use is \n",
    "\n",
    "$J(\\theta)\\:=\\:-\\frac{1}{N - 2m}\\sum_{i = m + 1}^{N - m} \\sum_{j = i-m \\wedge i \\ne j}^{i + m} log(P(w_j \\vert w_i))$\n",
    "\n",
    "However, based on the data we have it is not straightforward how to use this loss function\n",
    "\n",
    "\n",
    "We will first look at $P(w_j \\vert w_i)$, \n",
    "\n",
    "$w_i$ is a context word of the $n^{th}$ observation and represented as a one hot encoded vector of dimension V, $x_n$. Similarly, $w_j$ is our target vector and represented as one hot encoded vector $y_n$.\n",
    "\n",
    "$P(w_j \\vert w_i)\\:=\\: \\frac{exp(logit(x_i)_{w_j})}{\\sum_{k = 1}^v exp(logit(x_i)_{w_k})}$ \n",
    "\n",
    "$logit(x_i)$ are the raw outputs of dimension V generated by the neural network.\n",
    "The numerator of the above term corresponds to the logit scalar value corresponding to the index of word $w_j$ (the index of the none zero value in the one hot encoded vector $y_n$).\n",
    "\n",
    "For example, consider the sentence *I love NLP* and these three are the only words in our vocabulary.\n",
    "\n",
    "Now we have two inputs for context word love, *(love, I)* and *(love, NLP)*\n",
    "\n",
    "Assume that the one-hot encoded vectors are as follows\n",
    "\n",
    "love -> [1, 0, 0]\n",
    "\n",
    "I -> [0, 1, 0]\n",
    "\n",
    "NLP -> [0, 0, 1]\n",
    "\n",
    "and the logits when the word love given as input are [2, 10, 5], then following are the probabilities\n",
    "\n",
    "$P(love \\vert love)$ = exp(2) / exp(2) + exp(10) + exp(5) = 0.0003\n",
    "\n",
    "$P(I \\vert love)$ = exp(10) / exp(2) + exp(10) + exp(5) = 0.9930\n",
    "\n",
    "$P(NLP \\vert love)$ = exp(5) / exp(2) + exp(10) + exp(5) = 0.0067\n",
    "\n",
    "\n",
    "Coming back to the cost function, we saw\n",
    "\n",
    "$P(w_j \\vert w_i)\\:=\\: \\frac{exp(logit(x_i)_{w_j})}{\\sum_{k = 1}^v exp(logit(x_i)_{w_k})}$ \n",
    "\n",
    "therefore\n",
    "\n",
    "$log(P(w_j \\vert w_i))\\:=\\: logit(x_i)_{w_j} - log ({\\sum_{k = 1}^v exp(logit(x_i)_{w_k})})$ \n",
    "\n",
    "The cost function now becomes\n",
    "\n",
    "$J(\\theta)\\:=\\:-\\frac{1}{N - 2m}\\sum_{i = m + 1}^{N - m} \\sum_{j = i-m \\wedge i \\ne j}^{i + m} logit(x_i)_{w_j} - log ({\\sum_{k = 1}^v exp(logit(x_i)_{w_k})})$\n",
    "\n",
    "\n",
    "Continuing with the above example with sentence *I love NLP*, the loss when love is the center word is as follows\n",
    "\n",
    "$logit(x_i)_{w_j}$ = 10\n",
    "\n",
    "log ({\\sum_{k = 1}^v exp(logit(x_i)_{w_k})}) = log(exp(2) + exp(10) + exp(5)) = log(22182.26801) = 10.007\n",
    "\n",
    "\n",
    "Therefore the loss is -(10 - 10.007) = 0.007 (log is natural log)\n",
    "\n",
    "\n",
    "The cost function still is not practical as computing the loss for a single example will require us to calulate the logits for the entire vocabulary with the current context word. This can be seen from the $log ({\\sum_{k = 1}^v exp(logit(x_i)_{w_k})})$ in the loss function\n",
    "\n",
    "We therefore need a good approximation of an error function that is efficient.\n",
    "\n",
    "---\n",
    "\n",
    "The above formulation is right on paper, but we need a different approximation for the loss funntion used for training the network. The first approach we will look at is taking the **Negative Sampling** technique\n",
    "\n",
    "Softmax at the output layer for a vocabulary of size V is a multinomial classification with V possible classes. We do not want a proper probability of a context word given the target word but given a pair of words we should be able to classify whether it is a context word or noise for the given target word. We therefore convert this multinomial classification problem to a binomial classification problem. Refer to [this](https://arxiv.org/pdf/1410.8251.pdf) url for notes on NCE (**Noise Contrastive Estimation**)\n",
    "\n",
    "In negative sampling, let $p(D = 1 \\vert w, c)$ denote the probability that the pair (w, c) came from our training data, that is, this sample is a true target context word pair. Therefore $p(D = 0 \\vert w, c) = 1-  p(D = 0 \\vert w, c)$ is the probabilility that the pair (w, c) didn't come from training sample, that is it is a negative sample.\n",
    "\n",
    "The new goal therefore now given we are looking to train a binomial classifier detecting whether the given (w, c) pair is a target context word pair and our new objective is\n",
    "\n",
    "$argmax_{\\theta}\\prod_{(w, c) \\in D} p(D = 1\\vert w, c ;\\theta)$\n",
    "\n",
    "Taking log probability we have the following objective to maximise\n",
    "\n",
    "$argmax_{\\theta}\\sum_{(w, c) \\in D} log(p(D = 1\\vert w, c ;\\theta))$\n",
    "\n",
    "with the word embedding $v \\in R^d$ we can use a sigmoid function to predict \n",
    "\n",
    "$p(D = 1\\vert w, c; \\theta) = \\frac{1}{1 + e^{-v^t.v^w}} = \\sigma(v^t.v^w)$\n",
    "\n",
    "Therefore the new objective to minimize will be  \n",
    "\n",
    "$argmax_{\\theta}\\sum_{(w, c) \\in D} log(\\frac{1}{1 + e^{-v^t.v^w}})$\n",
    "\n",
    "This is trivial to solve as of the term $v^t.v^w$ is large, then we have the probability set to 1 and thus log probability will be 0. This thus will push all the word vector dot products to a high scalar value not achieving anything.\n",
    "\n",
    "To make the network differentiate these true samples from negative samples, we introduce some negative samples to the training.\n",
    "\n",
    "Thus we now change the objective function to accomodate these negative samples making our new objective to minimize \n",
    "\n",
    "$argmax_{\\theta}\\prod_{(w, c) \\in D} p(D = 1\\vert w, c ;\\theta) \\prod_{(w, c) \\in D^{\\prime}} p(D = 0\\vert w, c ;\\theta)$\n",
    "\n",
    "= $argmax_{\\theta}\\prod_{(w, c) \\in D} p(D = 1\\vert w, c ;\\theta) \\prod_{(w, c) \\in D^{\\prime}} 1 - p(D = 1\\vert w, c ;\\theta)$\n",
    "\n",
    "= $argmax_{\\theta}\\sum_{(w, c) \\in D} log(p(D = 1\\vert w, c ;\\theta)) + \\sum_{(w, c) \\in D^{\\prime}} log(1 - p(D = 1\\vert w, c ;\\theta))$\n",
    "\n",
    "Since we know\n",
    "\n",
    "$p(D = 1\\vert w, c; \\theta) =  \\sigma(v^t.v^w)$\n",
    "\n",
    "The function to minimize is \n",
    "\n",
    "= $argmax_{\\theta}\\sum_{(w, c) \\in D} log(\\sigma(v^t.v^w)) + \\sum_{(w, c) \\in D^{\\prime}} log(1 - \\sigma(v^t.v^w))$\n",
    "\n",
    "= $argmax_{\\theta}\\sum_{(w, c) \\in D} log(\\sigma(v^t.v^w)) + \\sum_{(w, c) \\in D^{\\prime}} log(\\sigma(-v^t.v^w))$\n",
    " \n",
    "as $\\sigma(1 - x) = \\sigma(-x)$\n",
    "\n",
    "\n",
    "One question remains for us to answer is how do we get $D^\\prime$ and how many negative samples (k) per positive sample do we add. The paper states k = 2 to 5 for large corpus and around 5 to 20 for small corpus.\n",
    "\n",
    "For selecting the negative sample, the sample is chosen from a unigram distribution with the probability of a word being selected is given by \n",
    "\n",
    "$p(w_i) = \\frac{f(w_i)^{3/4}}{\\sum_{j = 0}^{n}f(w_j)^{3/4}}$\n",
    "\n",
    "where $f(w_i)$ is the frequency of the word i in the corpus. More the count of this word, more is the probability of it being sampled. The power 3/4 is emperical and has no mathematical proof and it appears to have outperformed other function for the negative word sampling.\n",
    "\n",
    "Note: The paper at [this](https://arxiv.org/pdf/1402.3722v1.pdf) URL was helpful in understanding the negative sampling formulation. Also [this](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/) post is pretty useful and a good read\n",
    "\n",
    "---\n",
    "\n",
    "##### Hierarchical Softmax.\n",
    "\n",
    "The original probability $p(w_i|w_j)= \\frac{exp(logit(x_i)_{w_j})}{\\sum_{k = 1}^v exp(logit(x_i)_{w_k})}$ where $x_i$ is the one hot encoded vector of the target word. The computationally demanding part of this equation is the denominator which gets huge for large vocabulary and increases linearly with vocabulary size. Hierarchical softmax addressed this problem and brings this complexity from linear to logarithamic. Unlike Negative sampling hierarchical softmax doesn't require fake negative samples and only uses the true (target, context) pair.\n",
    "\n",
    "TODO: See this in more details with math and a real implementation.\n",
    "\n",
    "---\n",
    "\n",
    "We will implement skipgram using tensorflow, First, download the zip file corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File wikipedia2text-extracted.txt.bz2 already downloaded, using local copy\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlretrieve\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def maybe_download(url, filename):\n",
    "    if os.path.exists(filename):\n",
    "        print('File %s already downloaded, using local copy'%filename)\n",
    "    else:\n",
    "        #Not handling exceptions and missing file errors\n",
    "        print('Downloading file %s from %s'%(filename, url))\n",
    "        local_filename, headers = urlretrieve(url + '/' + filename)\n",
    "        shutil.move(local_filename, filename)\n",
    "    \n",
    "maybe_download('http://www.evanjones.ca/software','wikipedia2text-extracted.txt.bz2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets open and the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import nltk\n",
    "import os\n",
    "import math\n",
    "\n",
    "#Read the data 1 MB bytes at a time\n",
    "def read_corpus(archive):\n",
    "    file_size = os.stat(archive).st_size\n",
    "    chunk_size = 1024 * 1024\n",
    "    tokens = []\n",
    "    with bz2.open(archive) as f:\n",
    "        for i in range(math.ceil(file_size // chunk_size) + 1):\n",
    "            bytes_to_read = min(chunk_size, file_size - (i * chunk_size))\n",
    "            file_str = f.read(bytes_to_read).decode('utf-8').lower()\n",
    "            tokens.extend(nltk.word_tokenize(file_str))\n",
    "        \n",
    "    return tokens\n",
    "\n",
    "\n",
    "content = read_corpus('./wikipedia2text-extracted.txt.bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens are  3360286\n",
      "First 10 tokens are  ['propaganda', 'is', 'a', 'concerted', 'set', 'of', 'messages', 'aimed', 'at', 'influencing']\n",
      "Last 10 tokens are  ['favorable', 'long-term', 'outcomes', 'for', 'around', 'half', 'of', 'those', 'diagnosed', 'with']\n"
     ]
    }
   ],
   "source": [
    "print('Number of tokens are ', len(content))\n",
    "print('First 10 tokens are ', content[0:10])\n",
    "print('Last 10 tokens are ', content[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create the following 4 data structures\n",
    "\n",
    "- `dictionary`: Maps a string word to id creating a `(word, word_index)` pair.\n",
    "- `reverse_dictionary`: Maps a word index to word pair as `(word_index, word)`\n",
    "- `counts`: List of words and count pair as (word, num_occurance_of_word)\n",
    "- `data`: List of same size as content, but the word replaced with the index of the word\n",
    "\n",
    "Also, we will restrict the vocabulary to 50000 most common words. All other rare words will be replaced with `UNK`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top most common words in the corpus are [('UNK', -1), ('the', 226881), (',', 184013), ('.', 120944), ('of', 116323)]\n",
      "Sample data [1721, 9, 8, 16471, 223, 4, 5165, 4456, 26, 11590]\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "vocab_size = 50000\n",
    "\n",
    "def build_dataset(word_tokens):\n",
    "    counts = [('UNK', -1)]\n",
    "    counts.extend(collections.Counter(word_tokens).most_common(vocab_size - 1))\n",
    "    \n",
    "    dictionary = dict()\n",
    "    reverse_dictionary = dict()\n",
    "    idx = 0\n",
    "    for word, _ in counts:\n",
    "        dictionary[word] = idx\n",
    "        reverse_dictionary[idx] = word\n",
    "        idx += 1\n",
    "    #TODO: See if we need UNK's count\n",
    "    \n",
    "    data = [dictionary[word if word in dictionary else 'UNK'] for word in word_tokens]\n",
    "    \n",
    "    assert(len(dictionary) == vocab_size)\n",
    "    \n",
    "    return counts, dictionary, reverse_dictionary, data\n",
    "    \n",
    "counts, dictionary, reverse_dictionary, data = build_dataset(content)\n",
    "print('Top most common words in the corpus are', counts[0:5])\n",
    "print('Sample data', data[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now write the function that will generate batches of data for skipgram algorithm\n",
    "\n",
    "We will have two vectors returned, one for target word and the context word which forms the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_index = 0\n",
    "def generate_batch_skip_gram(batch_size, window_size):\n",
    "    global data_index\n",
    "    \n",
    "    batch_target = []\n",
    "    batch_context = []\n",
    "    \n",
    "    span = 2 * window_size + 1\n",
    "    deque = collections.deque(maxlen = span)\n",
    "    \n",
    "    for _ in range(span):\n",
    "        deque.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    \n",
    "    \n",
    "    for i in range(batch_size // (2 * window_size)):\n",
    "        batch_target.extend([deque[window_size]] * (window_size * 2))\n",
    "        labels = [deque[j] for j in list(range(window_size)) + list(range(window_size + 1, 2 * window_size + 1))]\n",
    "        batch_context.extend(labels)\n",
    "        deque.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "            \n",
    "    return np.array(batch_target), np.array(batch_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial 10 words in the data are  ['propaganda', 'is', 'a', 'concerted', 'set', 'of', 'messages', 'aimed', 'at', 'influencing']\n",
      "\n",
      "Batch target with window size: 1  is: ['is', 'is', 'a', 'a', 'concerted', 'concerted', 'set', 'set']\n",
      "Batch context with window size: 1  is: ['propaganda', 'a', 'is', 'concerted', 'a', 'set', 'concerted', 'of']\n",
      "Batch target with window size: 2  is: ['a', 'a', 'a', 'a', 'concerted', 'concerted', 'concerted', 'concerted']\n",
      "Batch context with window size: 2  is: ['propaganda', 'is', 'concerted', 'set', 'is', 'a', 'set', 'of']\n"
     ]
    }
   ],
   "source": [
    "print('Initial 10 words in the data are ', [reverse_dictionary[i] for i in data[:10]])\n",
    "print()\n",
    "for w in [1, 2]:\n",
    "    data_index = 0\n",
    "    batch_target, batch_context = generate_batch_skip_gram(batch_size = 8, window_size = w)\n",
    "    print('Batch target with window size:', w, ' is:', [reverse_dictionary[i] for i in batch_target])\n",
    "    print('Batch context with window size:', w, ' is:', [reverse_dictionary[i] for i in batch_context])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Skip-gram**\n",
    "\n",
    "We will now implement Skip-gram algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import tensorflow as tf\n",
    "import csv\n",
    "\n",
    "batch_size = 128\n",
    "embedding_dimension = 128\n",
    "window_size = 4\n",
    "\n",
    "#Words with lower index in are more frequent. \n",
    "#For example word with index 1 is more frequent than word with index 100\n",
    "#Select 16 from first 50 common words and 16 from 1000th to 1050th common words\n",
    "validation_words = random.sample(range(50), 16) + random.sample(range(1000, 1050), 16)\n",
    "\n",
    "\n",
    "#Reset tf graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#Input and labels placeholders\n",
    "x = tf.placeholder(name = 'input', dtype = tf.int32, shape = [batch_size])\n",
    "y = tf.placeholder(name = 'labels', dtype = tf.int32, shape = [batch_size, 1])\n",
    "\n",
    "\n",
    "#Constant for validation_words\n",
    "validation_set = tf.constant(validation_words, name = 'validation')\n",
    "\n",
    "\n",
    "#Embeddings\n",
    "embeddings = tf.Variable(\n",
    "            tf.random_uniform(shape = [vocab_size, embedding_dimension], \n",
    "                              dtype = tf.float32,\n",
    "                              minval = -1,\n",
    "                              maxval = 1), name = 'embeddings')\n",
    "\n",
    "\n",
    "#Softmax layer\n",
    "\n",
    "softmax_weights = tf.Variable(\n",
    "                        tf.truncated_normal(shape = [vocab_size, embedding_dimension],\n",
    "                                            dtype = tf.float32, \n",
    "                                            stddev = 0.5 / math.sqrt(embedding_dimension) #See why?\n",
    "                                           ), name = 'sm_weights')\n",
    "\n",
    "softmax_bias = tf.Variable(\n",
    "                        tf.random_uniform(shape = [vocab_size],\n",
    "                                          dtype = tf.float32,\n",
    "                                          minval = 0.0,\n",
    "                                          maxval = 0.01), name = 'sm_weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let us define the model\n",
    "\n",
    "TODO:\n",
    "For ``tf.nn.sampled_softmax_loss``, read [this](https://arxiv.org/abs/1412.2007)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Lookup embeddings from input\n",
    "embeds = tf.nn.embedding_lookup(embeddings, x)\n",
    "\n",
    "loss = tf.reduce_mean(\n",
    "            tf.nn.sampled_softmax_loss(\n",
    "                weights = softmax_weights,\n",
    "                biases = softmax_bias,\n",
    "                labels = y,\n",
    "                inputs = embeds,\n",
    "                num_sampled = 32,   #Negative samples\n",
    "                num_classes = vocab_size\n",
    "            )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Similarity metrics on validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "normalized_embeddings = embeddings / norm\n",
    "valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, validation_set)\n",
    "similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 2000 is  4.01424489063\n",
      "Average loss at step 4000 is  3.65001865935\n",
      "Average loss at step 6000 is  3.51581505644\n",
      "Average loss at step 8000 is  3.52141164279\n",
      "Average loss at step 10000 is  3.44985451114\n",
      "==================================================\n",
      "======= Validating against validation set =======\n",
      "====================================================================================================\n",
      "Nearest 5 words to a  are ['the', 'neo-renaissance', 'hardman', 'hebron', 'lengthwise']\n",
      "Nearest 5 words to of  are [',', \"'s\", 'in', '.', 'and']\n",
      "Nearest 5 words to was  are ['is', 'hostages', ',', 'had', 'ethéoir']\n",
      "Nearest 5 words to which  are ['tarot', 'secession', 'that', 'ct', 'leftists']\n",
      "Nearest 5 words to this  are ['it', 'omnipresence', 'zhao', 'trailers', 'loyalty']\n",
      "Nearest 5 words to on  are ['in', 'suburbs', 'bend', 'rejected', 'cellspacing=']\n",
      "Nearest 5 words to ;  are ['himalayas', 'populate', 'medusa', 'sacrum', 'standardized']\n",
      "Nearest 5 words to that  are ['it', 'warmth', 'caracas', 'coleridge', 'old']\n",
      "Nearest 5 words to have  are ['be', 'committed', '1751', 'had', 'mr']\n",
      "Nearest 5 words to to  are ['by', 'for', 'compliment', '41', 'and']\n",
      "Nearest 5 words to one  are ['accented', 'village', 'hibbard', 'scotland', 'wider']\n",
      "Nearest 5 words to with  are ['and', 'in', 'eta', ',', 'had']\n",
      "Nearest 5 words to the  are [',', 'and', 'in', 'a', '.']\n",
      "Nearest 5 words to as  are ['encircled', 'completed', 'laptop', 'corinthian', 'what']\n",
      "Nearest 5 words to their  are ['rhineland', 'powerpc', 'patti', 'prosperity', 'pauline']\n",
      "Nearest 5 words to city  are ['censuses', 'cultural', 'esophagus', 'chairperson', 'newman']\n",
      "Nearest 5 words to administration  are ['hiatus', 'argentino', 'continued', 'censored', 'remitted']\n",
      "Nearest 5 words to invasion  are ['kippur', 'crops', 'gurzuf', 'poesis', 'krasiński']\n",
      "Nearest 5 words to leaders  are ['mel', 'ven', 'instability', 'uav', '-|']\n",
      "Nearest 5 words to accepted  are ['mitsubishi', 'aptera', 'warrior', 'outliers', 'preserved']\n",
      "Nearest 5 words to methods  are ['laws', 'churchgate', 'party-list', '1.66', 'three-judge']\n",
      "Nearest 5 words to composed  are ['vocative', 'quotas', 'alia', 'assays', 'marshallese']\n",
      "Nearest 5 words to true  are ['afi', 'j-2', 'long-range', 'sculptural', 'conversely']\n",
      "Nearest 5 words to techniques  are ['conditionals', 'constituting', 'kendrick', 'apis', 'biodiesel']\n",
      "Nearest 5 words to points  are ['raft', 'berth', '14.3', 'omitting', 'miramanee']\n",
      "Nearest 5 words to cape  are ['shatigadud', 'underwood', 'taxis', 'homage', 'heaviest']\n",
      "Nearest 5 words to relationship  are ['controversial', 'north-west', 'shelved', 'whimsical', 'copepods']\n",
      "Nearest 5 words to knowledge  are ['wikitable', 'philae', 'silica', 'rainwater', 'orchestration']\n",
      "Nearest 5 words to washington  are ['gilla', 'attock', 'bracketed', 'đông', 'consumers']\n",
      "Nearest 5 words to organization  are ['replayed', 'screened', 'waterloo', 'audiences', 'zemin']\n",
      "Nearest 5 words to exist  are ['f.c.', 'miller', '1920.', 'equivalently', 'archeological']\n",
      "Nearest 5 words to 18  are ['hayyan', '700,000', 'affiliation', '2008.', 'everything']\n",
      "Average loss at step 12000 is  3.46214064449\n",
      "Average loss at step 14000 is  3.40592406011\n",
      "Average loss at step 16000 is  3.36020467579\n",
      "Average loss at step 18000 is  3.3779346664\n",
      "Average loss at step 20000 is  3.37763875043\n",
      "==================================================\n",
      "======= Validating against validation set =======\n",
      "====================================================================================================\n",
      "Nearest 5 words to a  are ['the', 'UNK', 'an', 'and', ',']\n",
      "Nearest 5 words to of  are ['in', '.', 'UNK', 'the', 'to']\n",
      "Nearest 5 words to was  are ['is', 'had', 'he', 'faltered', 'kure']\n",
      "Nearest 5 words to which  are ['that', 'gib', 'seddon', 'shells', 'but']\n",
      "Nearest 5 words to this  are ['it', 'omnipresence', 'an', 'mid-18th', 'muharram']\n",
      "Nearest 5 words to on  are ['in', 'penguin', 'from', 'árpád', ',']\n",
      "Nearest 5 words to ;  are ['.', 'resourceful', ',', '1271', 'enact']\n",
      "Nearest 5 words to that  are ['which', '10s', 'herald', 'it', 'integrating']\n",
      "Nearest 5 words to have  are ['are', 'be', 'has', 'had', 'giver']\n",
      "Nearest 5 words to to  are ['and', 'in', ',', '.', 'with']\n",
      "Nearest 5 words to one  are ['ackté', 'it', 'accented', 'year', 'certainly']\n",
      "Nearest 5 words to with  are ['and', 'for', 'to', '.', 'by']\n",
      "Nearest 5 words to the  are ['a', ',', 'in', '.', \"'s\"]\n",
      "Nearest 5 words to as  are ['sulphate', 'quipu', 'agatha', '1–2', 'sa']\n",
      "Nearest 5 words to their  are ['his', 'remnants', 'prosperity', 'motorcycles', 'doors']\n",
      "Nearest 5 words to city  are ['cultural', 'relegation', 'rod', 'barras', '730s']\n",
      "Nearest 5 words to administration  are ['hiatus', 'continued', 'argentino', 'censored', 'remitted']\n",
      "Nearest 5 words to invasion  are ['kippur', 'poesis', 'crops', 'gurzuf', 'her']\n",
      "Nearest 5 words to leaders  are ['mel', 'ven', 'instability', 'slick', 'f-75']\n",
      "Nearest 5 words to accepted  are ['mitsubishi', 'aptera', 'german-made', 'warrior', 'outliers']\n",
      "Nearest 5 words to methods  are ['laws', 'churchgate', 'party-list', '290', 'three-judge']\n",
      "Nearest 5 words to composed  are ['vocative', 'quotas', 'assays', 'marshallese', 'alia']\n",
      "Nearest 5 words to true  are ['afi', 'j-2', 'long-range', 'sculptural', 'abyssinians']\n",
      "Nearest 5 words to techniques  are ['constituting', 'conditionals', 'biodiesel', 'wilfred', 'magic']\n",
      "Nearest 5 words to points  are ['raft', 'berth', 'miramanee', '14.3', 'passport']\n",
      "Nearest 5 words to cape  are ['shatigadud', 'new', 'taxis', 'underwood', 'friend']\n",
      "Nearest 5 words to relationship  are ['controversial', 'north-west', 'safety', 'shelved', 'whimsical']\n",
      "Nearest 5 words to knowledge  are ['wikitable', 'philae', 'silica', 'rainwater', 'neo-colonial']\n",
      "Nearest 5 words to washington  are ['gilla', 'bracketed', 'attock', 'đông', 'consumers']\n",
      "Nearest 5 words to organization  are ['replayed', 'audiences', 'screened', 'waterloo', 'doe']\n",
      "Nearest 5 words to exist  are ['f.c.', 'miller', 'equivalently', 'archeological', '1920.']\n",
      "Nearest 5 words to 18  are ['hayyan', '700,000', 'affiliation', 'kenyan', '3-d']\n",
      "Average loss at step 22000 is  3.40351927245\n",
      "Average loss at step 24000 is  3.34569457769\n",
      "Average loss at step 26000 is  3.37356713009\n",
      "Average loss at step 28000 is  3.38867351043\n",
      "Average loss at step 30000 is  3.37731778669\n",
      "==================================================\n",
      "======= Validating against validation set =======\n",
      "====================================================================================================\n",
      "Nearest 5 words to a  are ['the', 'an', 'UNK', 'this', 'subtitled']\n",
      "Nearest 5 words to of  are [',', 'and', 'for', '.', 'UNK']\n",
      "Nearest 5 words to was  are ['became', 'offence', 'were', 'is', 'la']\n",
      "Nearest 5 words to which  are ['that', '.', 'but', ';', 'with']\n",
      "Nearest 5 words to this  are ['it', 'also', 'a', 'shortening', 'traditions']\n",
      "Nearest 5 words to on  are ['terribly', 'bronze', 'champ', 'upon', 'in']\n",
      "Nearest 5 words to ;  are ['which', '.', \"'voluntary\", 'but', 'initiative']\n",
      "Nearest 5 words to that  are ['which', 'but', 'kiwi', 'also', 'by']\n",
      "Nearest 5 words to have  are ['has', 'be', 'had', 'bursting', 'are']\n",
      "Nearest 5 words to to  are ['and', '.', ',', 'can', 'with']\n",
      "Nearest 5 words to one  are ['freak', 'sulawesi', 'wool', 'yarmouth', 'most']\n",
      "Nearest 5 words to with  are ['or', 'to', 'and', 'novoye', 'than']\n",
      "Nearest 5 words to the  are ['a', 'and', '.', ',', 'an']\n",
      "Nearest 5 words to as  are ['plats', 'foundation/wall', 'oulahan', 'hitting', 'disorders']\n",
      "Nearest 5 words to their  are ['its', 'the', 'powerpc', 'patti', 'claus']\n",
      "Nearest 5 words to city  are ['observation', 'rod', 'titanium', 'counties', 'chartered']\n",
      "Nearest 5 words to administration  are ['hiatus', 'argentino', 'continued', 'january-february', 'censored']\n",
      "Nearest 5 words to invasion  are ['kippur', 'poesis', 'crops', 'gurzuf', 'supplementing']\n",
      "Nearest 5 words to leaders  are ['mel', 'ven', 'instability', 'slick', 'uav']\n",
      "Nearest 5 words to accepted  are ['mitsubishi', 'aptera', 'german-made', 'preserved', 'warrior']\n",
      "Nearest 5 words to methods  are ['laws', 'party-list', 'girders', '290', 'churchgate']\n",
      "Nearest 5 words to composed  are ['vocative', 'quotas', 'marshallese', 'alia', 'assays']\n",
      "Nearest 5 words to true  are ['afi', 'j-2', 'long-range', 'sculptural', 'abyssinians']\n",
      "Nearest 5 words to techniques  are ['constituting', 'conditionals', 'wilfred', 'kendrick', 'biodiesel']\n",
      "Nearest 5 words to points  are ['dispositions', 'raft', '14.3', 'looting', 'latrobe']\n",
      "Nearest 5 words to cape  are ['shatigadud', 'taxis', 'underwood', 'friend', 'new']\n",
      "Nearest 5 words to relationship  are ['controversial', 'north-west', 'safety', 'opal', 'whimsical']\n",
      "Nearest 5 words to knowledge  are ['wikitable', 'philae', 'correlate', 'supports', 'rainwater']\n",
      "Nearest 5 words to washington  are ['gilla', 'đông', 'bracketed', 'attock', 'consumers']\n",
      "Nearest 5 words to organization  are ['replayed', 'moon', 'audiences', 'willard', 'screened']\n",
      "Nearest 5 words to exist  are ['f.c.', 'miller', 'equivalently', 'archeological', '1920.']\n",
      "Nearest 5 words to 18  are ['hayyan', '700,000', 'affiliation', 'kenyan', '3-d']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 32000 is  3.35534592664\n",
      "Average loss at step 34000 is  3.35331514645\n",
      "Average loss at step 36000 is  3.34478729099\n",
      "Average loss at step 38000 is  3.3584813081\n",
      "Average loss at step 40000 is  3.34987223971\n",
      "==================================================\n",
      "======= Validating against validation set =======\n",
      "====================================================================================================\n",
      "Nearest 5 words to a  are ['the', 'an', ',', '(', 'bologna']\n",
      "Nearest 5 words to of  are ['in', ',', '.', 'for', 'the']\n",
      "Nearest 5 words to was  are ['were', 'is', 'became', 'poking', 'chalk']\n",
      "Nearest 5 words to which  are ['but', 'tobolsk', 'that', 'improperly', 'single']\n",
      "Nearest 5 words to this  are ['it', \"'s\", 'that', 'a', 'after']\n",
      "Nearest 5 words to on  are ['villains', 'after', 'in', 'upon', 'widened']\n",
      "Nearest 5 words to ;  are ['.', '``', ':', ',', 'where']\n",
      "Nearest 5 words to that  are ['he', 'this', 'which', 'who', 'but']\n",
      "Nearest 5 words to have  are ['had', 'has', 'be', 'are', 'unfinished']\n",
      "Nearest 5 words to to  are ['.', ',', 'and', 'with', 'the']\n",
      "Nearest 5 words to one  are ['the', 'a', 'tranquilizer', 'marinated', '2.5']\n",
      "Nearest 5 words to with  are ['and', 'from', 'to', 'in', ',']\n",
      "Nearest 5 words to the  are ['a', ',', '.', 'and', \"'s\"]\n",
      "Nearest 5 words to as  are [',', 'one-day', 'and', 'psychiatrists', 'in']\n",
      "Nearest 5 words to their  are ['his', 'its', 'batchelor', 'threats', 'principalities']\n",
      "Nearest 5 words to city  are ['area', 'now', 'observation', 'counties', 'titanium']\n",
      "Nearest 5 words to administration  are ['hiatus', 'argentino', '2900', 'january-february', 'erfurt']\n",
      "Nearest 5 words to invasion  are ['kippur', 'poesis', 'supplementing', 'krasiński', 'taboos']\n",
      "Nearest 5 words to leaders  are ['mel', 'instability', 'ven', 'slick', 'f-75']\n",
      "Nearest 5 words to accepted  are ['mitsubishi', 'aptera', 'deuteride', 'german-made', 'preserved']\n",
      "Nearest 5 words to methods  are ['laws', 'party-list', '290', 'girders', 'rebellious']\n",
      "Nearest 5 words to composed  are ['vocative', 'marshallese', 'assays', 'quotas', 'alia']\n",
      "Nearest 5 words to true  are ['afi', 'j-2', 'codifications', 'abyssinians', 'long-range']\n",
      "Nearest 5 words to techniques  are ['conditionals', 'constituting', 'kendrick', 'wilfred', 'one-stringed']\n",
      "Nearest 5 words to points  are ['raft', 'cry', 'looting', 'passport', 'glutamate']\n",
      "Nearest 5 words to cape  are ['shatigadud', 'taxis', 'underwood', 'friend', 'homage']\n",
      "Nearest 5 words to relationship  are ['controversial', 'north-west', 'safety', 'whimsical', 'lysosomes']\n",
      "Nearest 5 words to knowledge  are ['wikitable', 'philae', 'supports', 'neo-colonial', 'silica']\n",
      "Nearest 5 words to washington  are ['gilla', 'bracketed', 'đông', 'attock', 'italian']\n",
      "Nearest 5 words to organization  are ['moon', 'replayed', 'zemin', 'willard', 'xué']\n",
      "Nearest 5 words to exist  are ['f.c.', 'archeological', 'miller', 'equivalently', '1920.']\n",
      "Nearest 5 words to 18  are ['hayyan', '700,000', 'affiliation', '3-d', 'kenyan']\n",
      "Average loss at step 42000 is  3.3695014329\n",
      "Average loss at step 44000 is  3.34805872977\n",
      "Average loss at step 46000 is  3.31847748071\n",
      "Average loss at step 48000 is  3.32647205126\n",
      "Average loss at step 50000 is  3.31485651171\n",
      "==================================================\n",
      "======= Validating against validation set =======\n",
      "====================================================================================================\n",
      "Nearest 5 words to a  are ['the', 'an', ',', 'is', 'noches']\n",
      "Nearest 5 words to of  are ['in', '.', 'with', ',', 'interdisciplinary']\n",
      "Nearest 5 words to was  are ['is', 'were', 'had', 'he', 'became']\n",
      "Nearest 5 words to which  are ['that', 'kindred', 'coffins', 'by', 'natalie']\n",
      "Nearest 5 words to this  are ['it', 'decomposed', 'ii-era', \"'s\", 'amboy']\n",
      "Nearest 5 words to on  are ['10–15', 'messmer', 'cleaners', 'hopping', 'plankton']\n",
      "Nearest 5 words to ;  are [':', '.', 'pushing', 'infantry', 'estates-general']\n",
      "Nearest 5 words to that  are ['which', 'he', 'what', 'not', 'also']\n",
      "Nearest 5 words to have  are ['had', 'be', 'are', 'has', 'looks']\n",
      "Nearest 5 words to to  are [',', 'and', '.', 'the', 'will']\n",
      "Nearest 5 words to one  are ['sulawesi', 'marinated', 'magnify', 'yarmouth', '2020.']\n",
      "Nearest 5 words to with  are ['and', 'or', 'of', '.', 'riemann']\n",
      "Nearest 5 words to the  are ['a', ',', '.', 'its', 'to']\n",
      "Nearest 5 words to as  are ['also', 'vizier', 'mosques', 'n1', 'conservation']\n",
      "Nearest 5 words to their  are ['his', 'its', '|mar_rec_hi_°f', 'they', '9th']\n",
      "Nearest 5 words to city  are ['now', 'new', 'insist', 'picking', 'lancaster']\n",
      "Nearest 5 words to administration  are ['hiatus', 'argentino', 'continued', '2900', 'censored']\n",
      "Nearest 5 words to invasion  are ['kippur', 'poesis', 'supplementing', 'krasiński', 'taboos']\n",
      "Nearest 5 words to leaders  are ['mel', 'instability', 'ven', 'incorporated', 'uav']\n",
      "Nearest 5 words to accepted  are ['mitsubishi', 'aptera', 'deuteride', 'neutron', 'warrior']\n",
      "Nearest 5 words to methods  are ['party-list', 'minami', 'laws', 'rebellious', '290']\n",
      "Nearest 5 words to composed  are ['marshallese', 'quotas', 'vocative', 'bohm', 'sequels']\n",
      "Nearest 5 words to true  are ['afi', 'codifications', 'j-2', 'polański', 'sculptural']\n",
      "Nearest 5 words to techniques  are ['constituting', 'conditionals', 'designed', 'kendrick', 'one-stringed']\n",
      "Nearest 5 words to points  are ['cry', 'looting', 'latrobe', 'raft', 'passport']\n",
      "Nearest 5 words to cape  are ['shatigadud', 'weevil', 'myskina', 'taxis', 'rich']\n",
      "Nearest 5 words to relationship  are ['controversial', 'north-west', 'safety', 'lysosomes', 'opal']\n",
      "Nearest 5 words to knowledge  are ['wikitable', 'grim', 'supports', 'correlate', 'neo-colonial']\n",
      "Nearest 5 words to washington  are ['gilla', 'bracketed', 'đông', 'attock', 'consumers']\n",
      "Nearest 5 words to organization  are ['moon', 'replayed', 'willard', 'zemin', 'audiences']\n",
      "Nearest 5 words to exist  are ['f.c.', 'archeological', 'sr', 'miller', 'tested']\n",
      "Nearest 5 words to 18  are ['700,000', 'hayyan', 'underlining', 'affiliation', 'fresh-water']\n",
      "Average loss at step 52000 is  3.32447648096\n",
      "Average loss at step 54000 is  3.29948104298\n",
      "Average loss at step 56000 is  3.28587788534\n",
      "Average loss at step 58000 is  3.32837161326\n",
      "Average loss at step 60000 is  3.30935981333\n",
      "==================================================\n",
      "======= Validating against validation set =======\n",
      "====================================================================================================\n",
      "Nearest 5 words to a  are ['an', 'the', 'bowie', 'its', 'which']\n",
      "Nearest 5 words to of  are ['for', 'the', 'city', '.', 'and']\n",
      "Nearest 5 words to was  are ['is', 'has', 'in', 'were', ',']\n",
      "Nearest 5 words to which  are ['that', 'and', 'but', 'a', 'when']\n",
      "Nearest 5 words to this  are ['it', 'nameless', 'his', 'further', 'fund']\n",
      "Nearest 5 words to on  are ['in', 'from', 'pál', 'diaspora', 'plot-related']\n",
      "Nearest 5 words to ;  are ['.', '(', 'danish', ':', 'elias']\n",
      "Nearest 5 words to that  are ['which', '.', 'it', 'when', 'muscles']\n",
      "Nearest 5 words to have  are ['had', 'has', 'are', 'interference', 'sparks']\n",
      "Nearest 5 words to to  are ['in', 'and', 'not-for-profit', 'for', 'with']\n",
      "Nearest 5 words to one  are ['most', 'at', 'unsaturated', 'three', 'royalist']\n",
      "Nearest 5 words to with  are ['from', 'and', 'to', 'in', 'ended']\n",
      "Nearest 5 words to the  are [',', \"'s\", 'its', 'a', 'of']\n",
      "Nearest 5 words to as  are ['agatha', 'rincón', 'deeper', 'hartwig', 'ajuuraan']\n",
      "Nearest 5 words to their  are ['many', 'its', 'supplies', 'them', 'his']\n",
      "Nearest 5 words to city  are ['area', 'of', 'des', 'museum', '.']\n",
      "Nearest 5 words to administration  are ['hiatus', 'argentino', '2900', 'avon', 'since']\n",
      "Nearest 5 words to invasion  are ['kippur', 'arrival', 'supplementing', 'krasiński', 'broadside']\n",
      "Nearest 5 words to leaders  are ['mel', 'instability', 'ven', 'uav', 'incorporated']\n",
      "Nearest 5 words to accepted  are ['mitsubishi', 'aptera', 'neutron', 'deuteride', 'preserved']\n",
      "Nearest 5 words to methods  are ['party-list', 'minami', 'urbain', '290', 'girders']\n",
      "Nearest 5 words to composed  are ['marshallese', 'bohm', 'vocative', 'quotas', 'dialed']\n",
      "Nearest 5 words to true  are ['afi', 'j-2', 'codifications', 'sculptural', 'achr']\n",
      "Nearest 5 words to techniques  are ['constituting', 'conditionals', 'one-stringed', 'designed', 'motherhood']\n",
      "Nearest 5 words to points  are ['cry', 'looting', 'latrobe', 'owing', 'i-70']\n",
      "Nearest 5 words to cape  are ['rich', 'shatigadud', 'taxis', 'seymour', 'tuamotu']\n",
      "Nearest 5 words to relationship  are ['controversial', 'safety', 'north-west', 'saarinen', 'opal']\n",
      "Nearest 5 words to knowledge  are ['wikitable', 'supports', 'grim', 'correlate', 'neo-colonial']\n",
      "Nearest 5 words to washington  are ['gilla', 'bracketed', 'attock', 'đông', 'consumers']\n",
      "Nearest 5 words to organization  are ['moon', 'willard', 'zemin', 'replayed', 'audiences']\n",
      "Nearest 5 words to exist  are ['f.c.', 'archeological', 'aunt', 'château', '1920.']\n",
      "Nearest 5 words to 18  are ['700,000', 'hayyan', 'affiliation', 'kenyan', '3-d']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 62000 is  3.34011850536\n",
      "Average loss at step 64000 is  3.31825440121\n",
      "Average loss at step 66000 is  3.31185459316\n",
      "Average loss at step 68000 is  3.28190550935\n",
      "Average loss at step 70000 is  3.2856021055\n",
      "==================================================\n",
      "======= Validating against validation set =======\n",
      "====================================================================================================\n",
      "Nearest 5 words to a  are ['the', 'an', 'any', 'and', ',']\n",
      "Nearest 5 words to of  are ['.', 'in', 'for', 'is', ',']\n",
      "Nearest 5 words to was  are ['is', 'are', 'were', 'be', 'camped']\n",
      "Nearest 5 words to which  are ['that', 'and', 'roddenberry', 'where', 'scots']\n",
      "Nearest 5 words to this  are ['its', 'it', 'the', 'also', 'a']\n",
      "Nearest 5 words to on  are ['upon', 'from', 'in', 'haste', 'device']\n",
      "Nearest 5 words to ;  are [':', '(', 'go-ahead', '11.8', ')']\n",
      "Nearest 5 words to that  are ['.', 'which', 'some', 'a', 'not']\n",
      "Nearest 5 words to have  are ['has', 'are', 'had', 'other', 'these']\n",
      "Nearest 5 words to to  are ['.', 'can', 'from', 'and', 'in']\n",
      "Nearest 5 words to one  are ['most', 'aec', 'lewis-clark', 'at', 'it']\n",
      "Nearest 5 words to with  are ['and', 'in', 'from', 'often', 'or']\n",
      "Nearest 5 words to the  are ['a', 'in', '.', ',', 'and']\n",
      "Nearest 5 words to as  are [',', 'often', 'schilder', 'and', 'able-bodied']\n",
      "Nearest 5 words to their  are ['them', 'its', 'these', 'many', 'other']\n",
      "Nearest 5 words to city  are ['capriati', 'country', 'area', 'northwest', 'piazza']\n",
      "Nearest 5 words to administration  are ['hiatus', 'argentino', '2900', 'january-february', 'avon']\n",
      "Nearest 5 words to invasion  are ['kippur', 'arrival', 'supplementing', 'benioff', 'krasiński']\n",
      "Nearest 5 words to leaders  are ['mel', 'instability', 'ven', 'deinonychus', 'incorporated']\n",
      "Nearest 5 words to accepted  are ['mitsubishi', 'aptera', 'sxt', 'thomas', 'german-made']\n",
      "Nearest 5 words to methods  are ['party-list', 'minami', 'girders', 'celebes', '290']\n",
      "Nearest 5 words to composed  are ['marshallese', 'bohm', 'enact', 'sequels', 'quotas']\n",
      "Nearest 5 words to true  are ['afi', 'j-2', 'codifications', 'billy', 'sculptural']\n",
      "Nearest 5 words to techniques  are ['constituting', 'conditionals', 'use', 'designed', 'kendrick']\n",
      "Nearest 5 words to points  are ['cry', 'looting', 'airfields', 'owing', '14.3']\n",
      "Nearest 5 words to cape  are ['taxis', 'rich', 'shatigadud', 'seymour', 'tuamotu']\n",
      "Nearest 5 words to relationship  are ['controversial', 'rocket', 'safety', 'north-west', 'opal']\n",
      "Nearest 5 words to knowledge  are ['correlate', 'wikitable', 'grim', '1879.', 'philae']\n",
      "Nearest 5 words to washington  are ['gilla', 'bracketed', 'attock', 'đông', 'consumers']\n",
      "Nearest 5 words to organization  are ['audiences', 'willard', 'zemin', 'moon', 'replayed']\n",
      "Nearest 5 words to exist  are ['f.c.', 'archeological', 'aunt', 'tested', 'transforms']\n",
      "Nearest 5 words to 18  are ['700,000', 'hayyan', '1', 'affiliation', 'paralleled']\n",
      "Average loss at step 72000 is  3.29633942711\n",
      "Average loss at step 74000 is  3.30572314155\n",
      "Average loss at step 76000 is  3.30443724573\n",
      "Average loss at step 78000 is  3.30153766918\n",
      "Average loss at step 80000 is  3.3135771147\n",
      "==================================================\n",
      "======= Validating against validation set =======\n",
      "====================================================================================================\n",
      "Nearest 5 words to a  are ['the', 'an', 'any', 'to', 'this']\n",
      "Nearest 5 words to of  are ['.', 'the', 'and', 'to', 'in']\n",
      "Nearest 5 words to was  are ['is', 'became', 'had', 'were', '26,000']\n",
      "Nearest 5 words to which  are ['that', 'hail', 'reared', 'and', 'but']\n",
      "Nearest 5 words to this  are ['it', 'however', 'an', 'a', 'its']\n",
      "Nearest 5 words to on  are ['upon', '10–15', 'moon', 'in', 'one-']\n",
      "Nearest 5 words to ;  are [':', 'racism', 'begin', '.', 'rancidity']\n",
      "Nearest 5 words to that  are ['.', 'which', 'if', 'not', 'when']\n",
      "Nearest 5 words to have  are ['has', 'had', 'decree', 'householder', 'readership']\n",
      "Nearest 5 words to to  are ['the', '.', 'and', 'of', 'spectators']\n",
      "Nearest 5 words to one  are ['most', 'shrinkage', 'mpande', ':', 'soul']\n",
      "Nearest 5 words to with  are ['and', 'or', 'in', ',', '.']\n",
      "Nearest 5 words to the  are ['a', ',', '.', 'of', 'and']\n",
      "Nearest 5 words to as  are ['non-physical', '1823.', 'and', 'sorry', 'supporting']\n",
      "Nearest 5 words to their  are ['them', 'many', 'her', 'doors', 'its']\n",
      "Nearest 5 words to city  are ['country', 'area', 'state', 'hoisting', 'capital']\n",
      "Nearest 5 words to administration  are ['hiatus', 'argentino', 'avon', '2900', 'january-february']\n",
      "Nearest 5 words to invasion  are ['kippur', 'arrival', 'supplementing', 'benioff', 'krasiński']\n",
      "Nearest 5 words to leaders  are ['mel', 'instability', 'f-75', 'ven', 'deviates']\n",
      "Nearest 5 words to accepted  are ['mitsubishi', 'sxt', 'aptera', 'thomas', 'crínán']\n",
      "Nearest 5 words to methods  are ['party-list', 'celebes', 'churchgate', '290', 'minami']\n",
      "Nearest 5 words to composed  are ['marshallese', 'bohm', 'sequels', 'enact', 'intercepted']\n",
      "Nearest 5 words to true  are ['j-2', 'afi', 'unwilling', 'know', 'merseyside']\n",
      "Nearest 5 words to techniques  are ['constituting', 'kendrick', 'conditionals', 'motherhood', 'designed']\n",
      "Nearest 5 words to points  are ['cry', 'looting', 'emaciated', 'airfields', 'latrobe']\n",
      "Nearest 5 words to cape  are ['taxis', 'rich', 'shatigadud', 'tuamotu', 'seymour']\n",
      "Nearest 5 words to relationship  are ['north-west', 'safety', 'rocket', 'saarinen', 'controversial']\n",
      "Nearest 5 words to knowledge  are ['grim', '1879.', 'wikitable', 'correlate', 'afrikaansche']\n",
      "Nearest 5 words to washington  are ['gilla', 'bracketed', 'đông', 'attock', 'jeter']\n",
      "Nearest 5 words to organization  are ['moon', '7,500,000', 'zemin', 'willard', 'replayed']\n",
      "Nearest 5 words to exist  are ['f.c.', 'archeological', 'transforms', 'tested', 'molybdenum']\n",
      "Nearest 5 words to 18  are ['700,000', 'hayyan', '1', 'kenyan', 'paralleled']\n",
      "Average loss at step 82000 is  3.26844063196\n",
      "Average loss at step 84000 is  3.26184415036\n",
      "Average loss at step 86000 is  3.2829081133\n",
      "Average loss at step 88000 is  3.28233989286\n",
      "Average loss at step 90000 is  3.25883336699\n",
      "==================================================\n",
      "======= Validating against validation set =======\n",
      "====================================================================================================\n",
      "Nearest 5 words to a  are ['the', 'an', 'no', 'this', '1981']\n",
      "Nearest 5 words to of  are ['in', ',', 'and', \"'s\", 'quartermess']\n",
      "Nearest 5 words to was  are ['is', 'were', 'had', 'became', 'has']\n",
      "Nearest 5 words to which  are ['with', 'but', 'that', 'by', 'asthmatic']\n",
      "Nearest 5 words to this  are ['it', 'its', 'also', 'an', 'a']\n",
      "Nearest 5 words to on  are ['in', ',', 'from', 'by', 'against']\n",
      "Nearest 5 words to ;  are [':', 'burner', '.', 'where', 'council-manager']\n",
      "Nearest 5 words to that  are ['because', 'which', 'when', '.', 'not']\n",
      "Nearest 5 words to have  are ['had', 'has', 'be', 'townspeople', 'post-industrial']\n",
      "Nearest 5 words to to  are ['and', 'monna', '.', 'would', ',']\n",
      "Nearest 5 words to one  are ['two', 'suffrage', 'three', 'yarmouth', 'most']\n",
      "Nearest 5 words to with  are ['had', 'and', 'which', 'against', 'while']\n",
      "Nearest 5 words to the  are ['a', 'and', ',', 'ramp', '.']\n",
      "Nearest 5 words to as  are ['or', 'conscientious', 'creole', 'usually', 'and']\n",
      "Nearest 5 words to their  are ['its', 'them', 'and', 'his', \"'s\"]\n",
      "Nearest 5 words to city  are ['capital', 'urban', 'furnishings', 'capriati', 'country']\n",
      "Nearest 5 words to administration  are ['hiatus', 'continued', 'argentino', 'avon', '2900']\n",
      "Nearest 5 words to invasion  are ['kippur', 'arrival', 'poisoned', 'supplementing', 'dravidian']\n",
      "Nearest 5 words to leaders  are ['mel', 'instability', 'f-75', 'ven', 'deinonychus']\n",
      "Nearest 5 words to accepted  are ['mitsubishi', 'sxt', 'aptera', 'thomas', '384']\n",
      "Nearest 5 words to methods  are ['celebes', '290', 'party-list', 'churchgate', 'girders']\n",
      "Nearest 5 words to composed  are ['marshallese', 'bohm', 'quotas', 'dialed', 'pairs']\n",
      "Nearest 5 words to true  are ['afi', 'j-2', 'unwilling', 'sculptural', 'accusing']\n",
      "Nearest 5 words to techniques  are ['constituting', 'kendrick', 'conditionals', 'motherhood', 'shines']\n",
      "Nearest 5 words to points  are ['cry', 'i-70', 'looting', '14.3', 'argentina']\n",
      "Nearest 5 words to cape  are ['taxis', 'rich', 'shatigadud', 'tuamotu', 'seymour']\n",
      "Nearest 5 words to relationship  are ['safety', 'north-west', 'controversial', 'rocket', 'opal']\n",
      "Nearest 5 words to knowledge  are ['afrikaansche', '1879.', 'grim', 'correlate', 'wikitable']\n",
      "Nearest 5 words to washington  are ['bracketed', 'attock', 'leading', 'đông', '22.1']\n",
      "Nearest 5 words to organization  are ['moon', 'zemin', 'audiences', '7,500,000', 'willard']\n",
      "Nearest 5 words to exist  are ['f.c.', 'molybdenum', 'transforms', 'aunt', 'archeological']\n",
      "Nearest 5 words to 18  are ['hayyan', '700,000', '1', 'kenyan', '½.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 92000 is  3.28744287515\n",
      "Average loss at step 94000 is  3.28927763528\n",
      "Average loss at step 96000 is  3.24522672433\n",
      "Average loss at step 98000 is  3.29689868844\n",
      "Average loss at step 100000 is  3.23581449604\n",
      "==================================================\n",
      "======= Validating against validation set =======\n",
      "====================================================================================================\n",
      "Nearest 5 words to a  are ['the', 'an', 'this', 'its', 'another']\n",
      "Nearest 5 words to of  are ['for', \"'s\", 'and', '.', 'to']\n",
      "Nearest 5 words to was  are ['is', 'became', 'were', 'had', 'has']\n",
      "Nearest 5 words to which  are ['but', 'that', 'where', 'while', 'electrons']\n",
      "Nearest 5 words to this  are ['it', 'an', 'a', 'its', 'another']\n",
      "Nearest 5 words to on  are ['in', ',', 'upon', 'and', 'from']\n",
      "Nearest 5 words to ;  are [':', 'hirelings', 'because', 'where', 'fillers']\n",
      "Nearest 5 words to that  are ['which', 'non-existent', '.', 'also', 'it']\n",
      "Nearest 5 words to have  are ['has', 'had', 'be', 'memorabilia', 'also']\n",
      "Nearest 5 words to to  are ['and', 'in', '.', 'the', ',']\n",
      "Nearest 5 words to one  are ['most', 'traceable', 'tithing', 'unmodified', 'important']\n",
      "Nearest 5 words to with  are ['and', ',', \"'s\", '.', 'to']\n",
      "Nearest 5 words to the  are ['a', 'and', \"'s\", ',', 'in']\n",
      "Nearest 5 words to as  are ['gsp', 'bamboo', 'nonfiction', 'fibrils', 'being']\n",
      "Nearest 5 words to their  are ['its', 'them', 'his', 'many', \"'s\"]\n",
      "Nearest 5 words to city  are ['district', 'area', 'museum', 'characterization', 'capital']\n",
      "Nearest 5 words to administration  are ['hiatus', 'erfurt', 'continued', '2900', 'avon']\n",
      "Nearest 5 words to invasion  are ['kippur', 'arrival', 'poisoned', 'supplementing', 'poesis']\n",
      "Nearest 5 words to leaders  are ['instability', 'mel', 'f-75', 'top-attack', 'incorporated']\n",
      "Nearest 5 words to accepted  are ['mitsubishi', 'sxt', 'sneakily', 'preserved', 'glucagon']\n",
      "Nearest 5 words to methods  are ['celebes', 'churchgate', 'party-list', 'dubuque', '290']\n",
      "Nearest 5 words to composed  are ['marshallese', 'bohm', 'gripped', 'dialed', 'enact']\n",
      "Nearest 5 words to true  are ['afi', 'j-2', 'sculptural', 'unwilling', 'word']\n",
      "Nearest 5 words to techniques  are ['constituting', 'kendrick', 'designed', 'shines', 'conditionals']\n",
      "Nearest 5 words to points  are ['cry', 'bonn', 'looting', 'i-70', '14.3']\n",
      "Nearest 5 words to cape  are ['rich', 'taxis', 'shatigadud', 'seymour', 'detached']\n",
      "Nearest 5 words to relationship  are ['safety', 'north-west', 'controversial', 'vnd', 'rocket']\n",
      "Nearest 5 words to knowledge  are ['afrikaansche', '1879.', 'grim', 'particular', 'specific']\n",
      "Nearest 5 words to washington  are ['đông', 'bracketed', 'attock', 'jeter', 'arabian']\n",
      "Nearest 5 words to organization  are ['moon', 'willard', 'audiences', 'zemin', 'xué']\n",
      "Nearest 5 words to exist  are ['f.c.', 'transforms', 'molybdenum', 'possible', 'archeological']\n",
      "Nearest 5 words to 18  are ['1', '700,000', 'hayyan', 'kenyan', 'paralleled']\n"
     ]
    }
   ],
   "source": [
    "num_steps = 100002\n",
    "skip_losses = []\n",
    "\n",
    "optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    cumulative_loss  = 0\n",
    "\n",
    "    for step in range(1, num_steps):\n",
    "        \n",
    "        #1. Get batch\n",
    "        batch_target, batch_context = generate_batch_skip_gram(\n",
    "                                        batch_size = batch_size,\n",
    "                                        window_size = window_size)\n",
    "        \n",
    "        feed_dict = {x : batch_target, y : batch_context.reshape(-1, 1)}\n",
    "        _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        \n",
    "        cumulative_loss += l\n",
    "        \n",
    "        if step % 2000 == 0:\n",
    "            mean_loss = cumulative_loss / 2000\n",
    "            print('Average loss at step', step, 'is ', mean_loss)\n",
    "            skip_losses.append(mean_loss)\n",
    "            cumulative_loss = 0\n",
    "            \n",
    "        if step % 10000 == 0:\n",
    "            print('='*50)\n",
    "            print('='*7, 'Validating against validation set', '='*7)\n",
    "            print('='*100)\n",
    "            sim = session.run(similarity)\n",
    "            for idx, validation_word in enumerate(validation_words):\n",
    "                v_word = reverse_dictionary[validation_word]\n",
    "                sim_vector = (-sim[idx, :]).argsort()[1:6] #Top 5\n",
    "                sim_words = [reverse_dictionary[i] for i in sim_vector]\n",
    "                print('Nearest 5 words to', v_word, ' are', sim_words)\n",
    "        \n",
    "    skip_gram_final_embeddings = normalized_embeddings.eval()        \n",
    "    \n",
    "np.save('skip_embeddings',skip_gram_final_embeddings)\n",
    "\n",
    "with open('skip_losses.csv', 'wt') as f:\n",
    "    writer = csv.writer(f, delimiter=',')\n",
    "    writer.writerow(skip_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
